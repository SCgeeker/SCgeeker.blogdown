<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sau-Chin Chen&#39;s website</title>
    <link>https://scchen.com/</link>
      <atom:link href="https://scchen.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Sau-Chin Chen&#39;s website</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 11 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://scchen.com/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Sau-Chin Chen&#39;s website</title>
      <link>https://scchen.com/</link>
    </image>
    
    <item>
      <title>Does Object Size Matter with Regard to the Mental Simulation of Object Orientation?</title>
      <link>https://scchen.com/publication/size-orientation/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/publication/size-orientation/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!---


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

---&gt;
&lt;p&gt;Access the sources at &lt;a href=&#34;https://curatescience.org/app/article/536&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Curate Science&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Orientation Effects across Languages</title>
      <link>https://scchen.com/project/mental_simulation/</link>
      <pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/project/mental_simulation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>If Emily Rosa met Thomas Bayes ...</title>
      <link>https://scchen.com/post/if-emily-rosa-met-thomas-bayes/</link>
      <pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/if-emily-rosa-met-thomas-bayes/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = FALSE
)
library(ggplot2)
library(gridExtra)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Twenty years ago 11-years-old Emily Rosa and her parents publsihed &lt;a href=&#34;https://jamanetwork.com/journals/jama/fullarticle/187390&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;their study&lt;/a&gt; in Journal of the American Medical Association. This study was designed by Emily herself two years before the publication. Before 1998, &lt;a href=&#34;https://en.wikipedia.org/wiki/Therapeutic_touch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Therapeutic Touch&lt;/a&gt; (commonly shortened to &amp;ldquo;TT&amp;rdquo;) had been advocated in hundreds of hospitals and nursing schools in North America. Forty thousands of health care professionals had been trained to use TT for their patient. The trained TT professionals were claimed, by the promoters, have the skill to manipulate &lt;strong&gt;the energy field surrounded human body&lt;/strong&gt; without touching. TT professionals just stoped the plams above the patients&#39; body for minutes to cure their pain and anxiety. However, the evidence about the reality of TT was vague.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;theraputic-touch.jpg&#34; alt=&#34;A TT therapist is healing a paitent.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Emily was curious how real was TT technique. To test their skill under unbiased observations, she designed the test table like the below figure shows. Every trainee would run 10 trials in one test. In each trial, Emily placed her right hand above one of the participated trainee&#39; hand, decided by flipped coin. She had the hypothesis if the skill of TT trainees was real, they would give the correct answers in at least 8 of the trials.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;joc71352f1.gif&#34; alt=&#34;Emily Rosa is testing her subjest.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Emily ran this study two times between 1996 and 1997. At the first time 15 TT trainees were invited to this study. She deiced to run the second time after the TV station interviewed her. At the second time 13 TT trainees, included 7 had participated in the first time, joined this study.  Their analysis showed the average accuracy of these trainees as equal to the random guessing.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/mNoRxCRJ-Y0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;We would learn many things from Emily&amp;rsquo;s study till today. First of all the critical finding was the null effect. Today the behavioral scientists share more attention about the null effect than 1998. The other interesting thing is that the prediction could be verified by one point. It is rare to use the one-sample t-test in the psychological studies, but almost every psychologist learned the one-sample t-test before the independent t-test and paired t-test. Latest but may be critical, the idea of little Emily embodied the statistical thinking.&lt;/p&gt;
&lt;p&gt;Emily&amp;rsquo;s data were summarized in the reproduced figure as below. They ran the one-sample t-test for the results of two times respectively. The analyses disconfirmed the reality of TT technique, but Emily and her parents might show a kind of researcher flexibility: they used the small samples in their first and follow-up studies. Could we upgrade their results in addition to conduct a registered replication plan?&lt;/p&gt;
&lt;p&gt;If Emily had learned Bayesian statistics in 1998, she would show the reporters her stronger belief how fake the TT technique was. Next, I use the binomial probability distribution to reanalyze Emily&amp;rsquo;s data in the simple Bayesian inference. My first step is the recovery of the original correct frequency of every TT participant. Although Emily&amp;rsquo;s paper gave the range of scores (initial study: 2 to 8; follow-up study: 1 to 7), they did not provide the how many correct trials conducted by every participant. I arranged the numbers based on the averages and number of total correct responses as follows:&lt;/p&gt;
&lt;p&gt;(R-chunk, vectors of numbers)&lt;/p&gt;
&lt;p&gt;Then I required the theoretical parameters representing Emily&amp;rsquo;s hypothesis. Her study in nature was a game of coin tossing. She assumed certificated TT participants, given their technique was real, could answer correctly in more than 8 trials. In the other word, among the ten coins flipped by Emily, they could look into which 8, 9, or all sides on the top. Before the initial study, Emily could assume the probability each participant give the correct answer in a trial to be more than 0.5. If she used Bayesian inference, she would expect the posterior probability higher than 0.8. Therefore I prepared the uniform distribution ranged between 0.5 and 1 as the prior probability of the Bayesian inference.&lt;/p&gt;
&lt;p&gt;It was recorded that only one participant returned 8 correct answers in the initial study. My Bayesian model used the binomial distribution with the arguments of 10 trials and prior probabilities. This model generated the posterior probability distribution given the data of Emily&amp;rsquo;s initial study. The average of the initial study was 4.67; thus I retrieved the posterior probability distribution of 5 from the Bayesian model.&lt;/p&gt;
&lt;!---
(R-chunk, Bayesian model and posterior histogram of initial)
---&gt;
&lt;p&gt;According to the posterior probability distribution of the initial study, the probability of TT trainees returned more than 8 correct answers would be &lt;code&gt;0.035&lt;/code&gt;. Because Emily had never considered the reproduction of her initial study before the interview, the follow-up study was like the second chance for the TT trainees. After the initial study, Emily updated her assumption of how many correct answers TT trainees could return in this study. I used the posterior probability distribution of the initial study as the prior probabilities for the analysis of the follow-up study. Then I built the Bayesian model and retrieved the posterior probability distribution given the average 4.06, I truncated to 4.&lt;/p&gt;
&lt;!---
(R-chunk, Bayesian model and posterior histogram of follow-up)
---&gt;
&lt;p&gt;The histogram of the posterior probability distribution shows zero possibility beyond 8. My reanalysis indicates that Emily&amp;rsquo;s study indeed falsified the reality of Therapeutic Touch. You may wonder if people have updated the understanding of Therapeutic Touch because of Emily&amp;rsquo;s study. Till today &lt;a href=&#34;http://therapeutictouch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the official website of Therapeutic Touch&lt;/a&gt; is regularly upgrading the latest information. More efforts are required to guide people thinking this world with scientific and statistical knowledge.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Psychological Science Podcasts</title>
      <link>https://scchen.com/post/ps-podcasts/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/ps-podcasts/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There are psychological scientists have started produce the internet podcasts since 2016. Audience will know the latest news and ideas because many hosts are open science promoters. Until now 8 podcasts are in my pocket. They are the treasure for whom want to know the true faces of scientists. Two days ago I have completed &lt;a href=&#34;http://scchen.com/zh/post/ps-podcasts.zh/&#34;&gt;Chinese version&lt;/a&gt;. I wish the scientific podcasts out of English will be online in coming months.&lt;/p&gt;
&lt;div id=&#34;everything-hertz&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Everything Hertz&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://scontent-tpe1-1.xx.fbcdn.net/v/t1.0-9/12801447_510072969180732_8893628584447559111_n.jpg?_nc_cat=0&amp;amp;oh=2e95e9204f865df4682b1cacfed12386&amp;amp;oe=5BD00652&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;host&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;host&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://jamesheathers.com/&#34;&gt;James Heathers&lt;/a&gt;；&lt;a href=&#34;http://www.dsquintana.com/&#34;&gt;Daniel Quintana&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;channel-website-social-media&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;channel, website, social media&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/everything-hertz&#34;&gt;SoundCloud&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://twitter.com/hertzpodcast&#34;&gt;twitter&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.facebook.com/everythinghertzpodcast&#34;&gt;Facebook&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;debut-date&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Debut date&lt;/h4&gt;
&lt;p&gt;February, 27 2016&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-favorite-episodes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;My favorite episodes&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/everything-hertz/44-whos-afraid-of-the-new-bad-people-with-nick-brown&#34;&gt;Nick Brown&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/everything-hertz/42-some-of-my-best-friends-are-bayesians-with-daniel-lakens&#34;&gt;Daniel Lakens, part 1&lt;/a&gt;;&lt;a href=&#34;https://soundcloud.com/everything-hertz/43-death-taxes-and-publication-bias-in-meta-analysis-with-daniel-lakens&#34;&gt;part 2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/everything-hertz/62-adopting-open-science-practices-with-dorothy-bishop&#34;&gt;Dorothy Bishop&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/everything-hertz/56-registered-reports-with-chris-chambers&#34;&gt;Chris Chambers&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-black-goat&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Black Goat&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://scontent-tpe1-1.xx.fbcdn.net/v/t1.0-9/16684080_255031914943271_624884278399716220_n.png?_nc_cat=0&amp;amp;oh=1c165d165607c2788cc15cdee2678649&amp;amp;oe=5BFA8C62&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;host-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;host&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://pages.uoregon.edu/sanjay/&#34;&gt;Sanjay Srivastava&lt;/a&gt;;&lt;a href=&#34;http://www.alexatullett.com/index.html&#34;&gt;Alexa Tullett&lt;/a&gt;;&lt;a href=&#34;http://www.simine.com/&#34;&gt;Simine Vazire&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;channel-website-social-media-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;channel, website, social media&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://itunes.apple.com/us/podcast/the-black-goat/id1217953035&#34;&gt;iTune&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://blackgoat.podbean.com/&#34;&gt;Podbean&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://www.theblackgoatpodcast.com/&#34;&gt;website&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://twitter.com/blackgoatpod&#34;&gt;twitter&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.facebook.com/blackgoatpod/&#34;&gt;Facebook&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.instagram.com/blackgoatpod/&#34;&gt;Instagram&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;debut-date-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Debut date&lt;/h4&gt;
&lt;p&gt;March, 22 2017&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-favorite-episodes-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;My favorite episodes&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://www.theblackgoatpodcast.com/posts/we-were-never-cool/&#34;&gt;We Were Never Cool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.theblackgoatpodcast.com/posts/i-felt-like-a-real-scientist/&#34;&gt;I Felt Like a Real Scientist&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.theblackgoatpodcast.com/posts/sipsapalooza/&#34;&gt;SIPSapalooza&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;circle-of-willis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Circle of Willis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://secureimg.stitcher.com/feedimageswide/480x270_149553.jpg&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;host-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;host&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://jamescoan.com/&#34;&gt;James Coan&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;channel-website-social-media-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;channel, website, social media&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://itunes.apple.com/us/podcast/circle-of-willis/id1277917115?ls=1&#34;&gt;iTune&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.stitcher.com/podcast/circle-of-willis?refid=stpr&#34;&gt;Stitcher&lt;/a&gt;
&lt;a href=&#34;http://circleofwillispodcast.com&#34;&gt;website&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://twitter.com/circle_ofwillis&#34;&gt;twitter&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.facebook.com/circleofwillispodcast/&#34;&gt;Facebook&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;debut-date-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Debut date&lt;/h4&gt;
&lt;p&gt;September, 8 2017&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-favorite-episodes-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;My favorite episodes&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://circleofwillispodcast.com/episode-1-introducing-circle-of-willis&#34;&gt;Introduction&lt;/a&gt;; connected with Taiwan and Title.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://circleofwillispodcast.com/episode-8-simine-vazire&#34;&gt;Simine Vazire&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-road-to-open-science&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Road to Open Science&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pbs.twimg.com/profile_images/996301283463192576/D4u7yTnf_400x400.jpg&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;host-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;host&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://openscience-utrecht.com/&#34;&gt;Open Science Community Utrecht&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;channel-website-social-media-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;channel, website, social media&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/utrechtyoungacademy&#34;&gt;SoundCloud&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://openscience-utrecht.com/oscu-podcast/&#34;&gt;website&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://twitter.com/R2OSpodcast&#34;&gt;twitter&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;debut-date-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Debut date&lt;/h4&gt;
&lt;p&gt;April, 12 2018&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-favorite-episodes-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;My favorite episodes&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/utrechtyoungacademy/the-road-to-open-science-ep2b-full-interview-with-daniel-lakens&#34;&gt;Daniel Lakens&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;two-psychologists-four-beers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two Psychologists Four Beers&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://images.fireside.fm/podcasts/images/6/69da8ae3-a19e-41ed-a678-0e145a936a3f/cover_small.jpg&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;host-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;host&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://yoelinbar.net/&#34;&gt;Yoel Inbar&lt;/a&gt;;&lt;a href=&#34;http://michaelinzlicht.com&#34;&gt;Michael Inzlicht&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;channel-website-social-media-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;channel, website, social media&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://itunes.apple.com/us/podcast/two-psychologists-four-beers/id1387529624&#34;&gt;iTune&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.stitcher.com/podcast/two-psychologists-four-beers?refid=stpr&#34;&gt;Stitcher&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://fourbeers.fireside.fm/&#34;&gt;website&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://twitter.com/fourbeerspod&#34;&gt;twitter&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;debut-date-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Debut date&lt;/h4&gt;
&lt;p&gt;May, 19 2018&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-favorite-episodes-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;My favorite episodes&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://fourbeers.fireside.fm/4&#34;&gt;The Replication Crisis Gets Personal&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-bayes-factor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Bayes Factor&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://scontent-tpe1-1.xx.fbcdn.net/v/t1.0-9/23172689_1993255780888928_6201212654157655772_n.png?_nc_cat=0&amp;amp;oh=a363fd368a6989930ccf191bdb59d266&amp;amp;oe=5C0F6877&#34; style=&#34;width:30.0%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;host-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;host&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.tufts.edu/hilab/people/&#34;&gt;JP De Ruiter&lt;/a&gt;;&lt;a href=&#34;https://alexanderetz.com/&#34;&gt;Alex Etz&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;channel-website-social-media-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;channel, website, social media&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://itunes.apple.com/us/podcast/the-bayes-factor/id1308207723&#34;&gt;iTune&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://sites.tufts.edu/hilab/podcast/&#34;&gt;website&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://twitter.com/TheBayesFactor&#34;&gt;twitter&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.facebook.com/TheBayesFactor/&#34;&gt;Facebook&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;debut-date-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Debut date&lt;/h4&gt;
&lt;p&gt;November, 5 2017&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducibiliteapod&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ReproducibiliTeaPod&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://pbs.twimg.com/profile_images/1008816656406269952/FCYhhxX7_400x400.jpg&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;host-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;host&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.psy.ox.ac.uk/team/amy-orben&#34;&gt;Amy Orben&lt;/a&gt;;&lt;a href=&#34;http://samdparsons.blogspot.com/&#34;&gt;Sam Parsons&lt;/a&gt;;&lt;a href=&#34;https://twitter.com/Cruwelli&#34;&gt;Sophia Crüwell&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;channel-website-social-media-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;channel, website, social media&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/reproducibilitea&#34;&gt;SoundCloud&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://twitter.com/ReproducibiliT&#34;&gt;twitter&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;debut-date-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Debut date&lt;/h4&gt;
&lt;p&gt;June, 27 2018&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-favorite-episodes-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;My favorite episodes&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/reproducibilitea/episode-3-questionable-research-practices&#34;&gt;Questionable Research Practices&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/reproducibilitea/episode-4-reproducibility-now&#34;&gt;Reproducibility Now&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;physiology-behavior-podcast&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Physiology &amp;amp; Behavior podcast&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/pippa/image/fetch/h_500,w_500/https://assets.pippa.io/shows/5b33637a7b9cf5a0177f18b5/1531915490998-fafca19bfd0d7c6f179e0b0403be84f6.jpeg&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;host-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;host&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://www.dsquintana.com/&#34;&gt;Daniel Quintana&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;channel-website-social-media-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;channel, website, social media&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://shows.pippa.io/dsquintana&#34;&gt;website&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://twitter.com/PB_cast&#34;&gt;twitter&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.facebook.com/dsquintana.research/&#34;&gt;Facebook&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.instagram.com/dsquintana&#34;&gt;Instagram&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;debut-date-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Debut date&lt;/h4&gt;
&lt;p&gt;July, 15 2018&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://scchen.com/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://scchen.com/privacy/</guid>
      <description>&lt;!---
Add your privacy policy here and set `draft: false` to publish it. Otherwise, delete this file if you don&#39;t need it.
---&gt;
&lt;p&gt;Last updated: December 14, 2020&lt;/p&gt;
&lt;p&gt;This Privacy Policy describes Our policies and procedures on the
collection, use and disclosure of Your information when You use the
Service and tells You about Your privacy rights and how the law protects
You.&lt;/p&gt;
&lt;p&gt;We use Your Personal data to provide and improve the Service. By using
the Service, You agree to the collection and use of information in
accordance with this Privacy Policy. This Privacy Policy has been
created with the help of the &lt;a href=&#34;https://www.termsfeed.com/privacy-policy-generator/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Privacy Policy
Generator&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;interpretation-and-definitions&#34;&gt;Interpretation and Definitions&lt;/h1&gt;
&lt;h2 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;The words of which the initial letter is capitalized have meanings
defined under the following conditions. The following definitions shall
have the same meaning regardless of whether they appear in singular or
in plural.&lt;/p&gt;
&lt;h2 id=&#34;definitions&#34;&gt;Definitions&lt;/h2&gt;
&lt;p&gt;For the purposes of this Privacy Policy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Account&lt;/strong&gt; means a unique account created for You to access our
Service or parts of our Service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Company&lt;/strong&gt; (referred to as either &amp;ldquo;the Company&amp;rdquo;, &amp;ldquo;We&amp;rdquo;, &amp;ldquo;Us&amp;rdquo; or
&amp;ldquo;Our&amp;rdquo; in this Agreement) refers to Sau-Chin Chen.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cookies&lt;/strong&gt; are small files that are placed on Your computer, mobile
device or any other device by a website, containing the details of
Your browsing history on that website among its many uses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Country&lt;/strong&gt; refers to: Taiwan&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Device&lt;/strong&gt; means any device that can access the Service such as a
computer, a cellphone or a digital tablet.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Personal Data&lt;/strong&gt; is any information that relates to an identified
or identifiable individual.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Service&lt;/strong&gt; refers to the Website.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Service Provider&lt;/strong&gt; means any natural or legal person who processes
the data on behalf of the Company. It refers to third-party
companies or individuals employed by the Company to facilitate the
Service, to provide the Service on behalf of the Company, to perform
services related to the Service or to assist the Company in
analyzing how the Service is used.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Third-party Social Media Service&lt;/strong&gt; refers to any website or any
social network website through which a User can log in or create an
account to use the Service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Usage Data&lt;/strong&gt; refers to data collected automatically, either
generated by the use of the Service or from the Service
infrastructure itself (for example, the duration of a page visit).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Website&lt;/strong&gt; refers to Sau-Chin Chen, accessible from
&lt;a href=&#34;https://scchen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://scchen.com&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;You&lt;/strong&gt; means the individual accessing or using the Service, or the
company, or other legal entity on behalf of which such individual is
accessing or using the Service, as applicable.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;collecting-and-using-your-personal-data&#34;&gt;Collecting and Using Your Personal Data&lt;/h1&gt;
&lt;h2 id=&#34;types-of-data-collected&#34;&gt;Types of Data Collected&lt;/h2&gt;
&lt;h3 id=&#34;personal-data&#34;&gt;Personal Data&lt;/h3&gt;
&lt;p&gt;While using Our Service, We may ask You to provide Us with certain
personally identifiable information that can be used to contact or
identify You. Personally identifiable information may include, but is
not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Email address&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Usage Data&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;usage-data&#34;&gt;Usage Data&lt;/h3&gt;
&lt;p&gt;Usage Data is collected automatically when using the Service.&lt;/p&gt;
&lt;p&gt;Usage Data may include information such as Your Device&amp;rsquo;s Internet
Protocol address (e.g. IP address), browser type, browser version, the
pages of our Service that You visit, the time and date of Your visit,
the time spent on those pages, unique device identifiers and other
diagnostic data.&lt;/p&gt;
&lt;p&gt;When You access the Service by or through a mobile device, We may
collect certain information automatically, including, but not limited
to, the type of mobile device You use, Your mobile device unique ID, the
IP address of Your mobile device, Your mobile operating system, the type
of mobile Internet browser You use, unique device identifiers and other
diagnostic data.&lt;/p&gt;
&lt;p&gt;We may also collect information that Your browser sends whenever You
visit our Service or when You access the Service by or through a mobile
device.&lt;/p&gt;
&lt;h3 id=&#34;tracking-technologies-and-cookies&#34;&gt;Tracking Technologies and Cookies&lt;/h3&gt;
&lt;p&gt;We use Cookies and similar tracking technologies to track the activity
on Our Service and store certain information. Tracking technologies used
are beacons, tags, and scripts to collect and track information and to
improve and analyze Our Service. The technologies We use may include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cookies or Browser Cookies.&lt;/strong&gt; A cookie is a small file placed on
Your Device. You can instruct Your browser to refuse all Cookies or
to indicate when a Cookie is being sent. However, if You do not
accept Cookies, You may not be able to use some parts of our
Service. Unless you have adjusted Your browser setting so that it
will refuse Cookies, our Service may use Cookies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flash Cookies.&lt;/strong&gt; Certain features of our Service may use local
stored objects (or Flash Cookies) to collect and store information
about Your preferences or Your activity on our Service. Flash
Cookies are not managed by the same browser settings as those used
for Browser Cookies. For more information on how You can delete
Flash Cookies, please read &amp;ldquo;Where can I change the settings for
disabling, or deleting local shared objects?&amp;rdquo; available at
&lt;a href=&#34;https://helpx.adobe.com/flash-player/kb/disable-local-shared-objects-flash.html#main_Where_can_I_change_the_settings_for_disabling__or_deleting_local_shared_objects_&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://helpx.adobe.com/flash-player/kb/disable-local-shared-objects-flash.html#main_Where_can_I_change_the_settings_for_disabling__or_deleting_local_shared_objects_&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Web Beacons.&lt;/strong&gt; Certain sections of our Service and our emails may
contain small electronic files known as web beacons (also referred
to as clear gifs, pixel tags, and single-pixel gifs) that permit the
Company, for example, to count users who have visited those pages or
opened an email and for other related website statistics (for
example, recording the popularity of a certain section and verifying
system and server integrity).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cookies can be &amp;ldquo;Persistent&amp;rdquo; or &amp;ldquo;Session&amp;rdquo; Cookies. Persistent Cookies
remain on Your personal computer or mobile device when You go offline,
while Session Cookies are deleted as soon as You close Your web browser.
You can learn more about cookies here: &lt;a href=&#34;https://www.termsfeed.com/blog/cookies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;All About Cookies by
TermsFeed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We use both Session and Persistent Cookies for the purposes set out
below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Necessary / Essential Cookies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Type: Session Cookies&lt;/p&gt;
&lt;p&gt;Administered by: Us&lt;/p&gt;
&lt;p&gt;Purpose: These Cookies are essential to provide You with services
available through the Website and to enable You to use some of its
features. They help to authenticate users and prevent fraudulent use
of user accounts. Without these Cookies, the services that You have
asked for cannot be provided, and We only use these Cookies to
provide You with those services.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cookies Policy / Notice Acceptance Cookies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Type: Persistent Cookies&lt;/p&gt;
&lt;p&gt;Administered by: Us&lt;/p&gt;
&lt;p&gt;Purpose: These Cookies identify if users have accepted the use of
cookies on the Website.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Functionality Cookies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Type: Persistent Cookies&lt;/p&gt;
&lt;p&gt;Administered by: Us&lt;/p&gt;
&lt;p&gt;Purpose: These Cookies allow us to remember choices You make when
You use the Website, such as remembering your login details or
language preference. The purpose of these Cookies is to provide You
with a more personal experience and to avoid You having to re-enter
your preferences every time You use the Website.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information about the cookies we use and your choices regarding
cookies, please visit our Cookies Policy or the Cookies section of our
Privacy Policy.&lt;/p&gt;
&lt;h2 id=&#34;use-of-your-personal-data&#34;&gt;Use of Your Personal Data&lt;/h2&gt;
&lt;p&gt;The Company may use Personal Data for the following purposes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;To provide and maintain our Service&lt;/strong&gt;, including to monitor the
usage of our Service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;To manage Your Account:&lt;/strong&gt; to manage Your registration as a user of
the Service. The Personal Data You provide can give You access to
different functionalities of the Service that are available to You
as a registered user.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;For the performance of a contract:&lt;/strong&gt; the development, compliance
and undertaking of the purchase contract for the products, items or
services You have purchased or of any other contract with Us through
the Service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;To contact You:&lt;/strong&gt; To contact You by email, telephone calls, SMS,
or other equivalent forms of electronic communication, such as a
mobile application&amp;rsquo;s push notifications regarding updates or
informative communications related to the functionalities, products
or contracted services, including the security updates, when
necessary or reasonable for their implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;To provide You&lt;/strong&gt; with news, special offers and general information
about other goods, services and events which we offer that are
similar to those that you have already purchased or enquired about
unless You have opted not to receive such information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;To manage Your requests:&lt;/strong&gt; To attend and manage Your requests to
Us.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;For business transfers:&lt;/strong&gt; We may use Your information to evaluate
or conduct a merger, divestiture, restructuring, reorganization,
dissolution, or other sale or transfer of some or all of Our assets,
whether as a going concern or as part of bankruptcy, liquidation, or
similar proceeding, in which Personal Data held by Us about our
Service users is among the assets transferred.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;For other purposes&lt;/strong&gt;: We may use Your information for other
purposes, such as data analysis, identifying usage trends,
determining the effectiveness of our promotional campaigns and to
evaluate and improve our Service, products, services, marketing and
your experience.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We may share Your personal information in the following situations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;With Service Providers:&lt;/strong&gt; We may share Your personal information
with Service Providers to monitor and analyze the use of our
Service, to contact You.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For business transfers:&lt;/strong&gt; We may share or transfer Your personal
information in connection with, or during negotiations of, any
merger, sale of Company assets, financing, or acquisition of all or
a portion of Our business to another company.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;With Affiliates:&lt;/strong&gt; We may share Your information with Our
affiliates, in which case we will require those affiliates to honor
this Privacy Policy. Affiliates include Our parent company and any
other subsidiaries, joint venture partners or other companies that
We control or that are under common control with Us.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;With business partners:&lt;/strong&gt; We may share Your information with Our
business partners to offer You certain products, services or
promotions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;With other users:&lt;/strong&gt; when You share personal information or
otherwise interact in the public areas with other users, such
information may be viewed by all users and may be publicly
distributed outside. If You interact with other users or register
through a Third-Party Social Media Service, Your contacts on the
Third-Party Social Media Service may see Your name, profile,
pictures and description of Your activity. Similarly, other users
will be able to view descriptions of Your activity, communicate with
You and view Your profile.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;With Your consent&lt;/strong&gt;: We may disclose Your personal information for
any other purpose with Your consent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;retention-of-your-personal-data&#34;&gt;Retention of Your Personal Data&lt;/h2&gt;
&lt;p&gt;The Company will retain Your Personal Data only for as long as is
necessary for the purposes set out in this Privacy Policy. We will
retain and use Your Personal Data to the extent necessary to comply with
our legal obligations (for example, if we are required to retain your
data to comply with applicable laws), resolve disputes, and enforce our
legal agreements and policies.&lt;/p&gt;
&lt;p&gt;The Company will also retain Usage Data for internal analysis purposes.
Usage Data is generally retained for a shorter period of time, except
when this data is used to strengthen the security or to improve the
functionality of Our Service, or We are legally obligated to retain this
data for longer time periods.&lt;/p&gt;
&lt;h2 id=&#34;transfer-of-your-personal-data&#34;&gt;Transfer of Your Personal Data&lt;/h2&gt;
&lt;p&gt;Your information, including Personal Data, is processed at the Company&amp;rsquo;s
operating offices and in any other places where the parties involved in
the processing are located. It means that this information may be
transferred to — and maintained on — computers located outside of Your
state, province, country or other governmental jurisdiction where the
data protection laws may differ than those from Your jurisdiction.&lt;/p&gt;
&lt;p&gt;Your consent to this Privacy Policy followed by Your submission of such
information represents Your agreement to that transfer.&lt;/p&gt;
&lt;p&gt;The Company will take all steps reasonably necessary to ensure that Your
data is treated securely and in accordance with this Privacy Policy and
no transfer of Your Personal Data will take place to an organization or
a country unless there are adequate controls in place including the
security of Your data and other personal information.&lt;/p&gt;
&lt;h2 id=&#34;disclosure-of-your-personal-data&#34;&gt;Disclosure of Your Personal Data&lt;/h2&gt;
&lt;h3 id=&#34;business-transactions&#34;&gt;Business Transactions&lt;/h3&gt;
&lt;p&gt;If the Company is involved in a merger, acquisition or asset sale, Your
Personal Data may be transferred. We will provide notice before Your
Personal Data is transferred and becomes subject to a different Privacy
Policy.&lt;/p&gt;
&lt;h3 id=&#34;law-enforcement&#34;&gt;Law enforcement&lt;/h3&gt;
&lt;p&gt;Under certain circumstances, the Company may be required to disclose
Your Personal Data if required to do so by law or in response to valid
requests by public authorities (e.g. a court or a government agency).&lt;/p&gt;
&lt;h3 id=&#34;other-legal-requirements&#34;&gt;Other legal requirements&lt;/h3&gt;
&lt;p&gt;The Company may disclose Your Personal Data in the good faith belief
that such action is necessary to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Comply with a legal obligation&lt;/li&gt;
&lt;li&gt;Protect and defend the rights or property of the Company&lt;/li&gt;
&lt;li&gt;Prevent or investigate possible wrongdoing in connection with the
Service&lt;/li&gt;
&lt;li&gt;Protect the personal safety of Users of the Service or the public&lt;/li&gt;
&lt;li&gt;Protect against legal liability&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;security-of-your-personal-data&#34;&gt;Security of Your Personal Data&lt;/h2&gt;
&lt;p&gt;The security of Your Personal Data is important to Us, but remember that
no method of transmission over the Internet, or method of electronic
storage is 100% secure. While We strive to use commercially acceptable
means to protect Your Personal Data, We cannot guarantee its absolute
security.&lt;/p&gt;
&lt;h1 id=&#34;links-to-other-websites&#34;&gt;Links to Other Websites&lt;/h1&gt;
&lt;p&gt;Our Service may contain links to other websites that are not operated by
Us. If You click on a third party link, You will be directed to that
third party&amp;rsquo;s site. We strongly advise You to review the Privacy Policy
of every site You visit.&lt;/p&gt;
&lt;p&gt;We have no control over and assume no responsibility for the content,
privacy policies or practices of any third party sites or services.&lt;/p&gt;
&lt;h1 id=&#34;changes-to-this-privacy-policy&#34;&gt;Changes to this Privacy Policy&lt;/h1&gt;
&lt;p&gt;We may update Our Privacy Policy from time to time. We will notify You
of any changes by posting the new Privacy Policy on this page.&lt;/p&gt;
&lt;p&gt;We will let You know via email and/or a prominent notice on Our Service,
prior to the change becoming effective and update the &amp;ldquo;Last updated&amp;rdquo;
date at the top of this Privacy Policy.&lt;/p&gt;
&lt;p&gt;You are advised to review this Privacy Policy periodically for any
changes. Changes to this Privacy Policy are effective when they are
posted on this page.&lt;/p&gt;
&lt;h1 id=&#34;contact-us&#34;&gt;Contact Us&lt;/h1&gt;
&lt;p&gt;If you have any questions about this Privacy Policy, You can contact us:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By email: &lt;a href=&#34;https://scchen.com/cdn-cgi/l/email-protection&#34;&gt;[email protected]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Use bookdownplus write papers and manage lab logs</title>
      <link>https://scchen.com/post/use-bookdownplus/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/use-bookdownplus/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Since being a Rmarkdown user, I’m thinking how to write papers, manage teaching materials and record lab works in this format. When I met &lt;a href=&#34;https://github.com/crsh/papaja&#34;&gt;&lt;code&gt;papaja&lt;/code&gt;&lt;/a&gt;&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-AustpapajaReproducibleAPA2017&#34; role=&#34;doc-biblioref&#34;&gt;Aust and Barth 2017&lt;/a&gt;)&lt;/span&gt; three years ago, it is my dream someday I will use &lt;code&gt;R&lt;/code&gt;&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-RDevelopmentCoreTeamlanguageenvironmentstatistical2010&#34; role=&#34;doc-biblioref&#34;&gt;R Development Core Team 2010&lt;/a&gt;)&lt;/span&gt; process all my works. &lt;code&gt;papaja&lt;/code&gt; is amazing but it is implemented to English only. Later I found &lt;a href=&#34;https://bookdown.org/yihui/bookdown/get-started.html&#34;&gt;&lt;code&gt;bookdown&lt;/code&gt;&lt;/a&gt; developed by Yihui Xie&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-XiebookdownAuthoringBooks2017&#34; role=&#34;doc-biblioref&#34;&gt;Xie 2017&lt;/a&gt;)&lt;/span&gt;. &lt;code&gt;bookdown&lt;/code&gt; has the potential to manage Chinese writings. However, this package is primarily developed for writing and publishing books.&lt;/p&gt;
&lt;p&gt;Fortunately, &lt;a href=&#34;http://www.pzhao.org/zh/&#34;&gt;Peng Zhao&lt;/a&gt; has worked on the advanced work: &lt;a href=&#34;https://github.com/pzhaonet/bookdownplus&#34;&gt;&lt;code&gt;bookdownplus&lt;/code&gt;&lt;/a&gt;&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ZhaobookdownplusTextbook2017&#34; role=&#34;doc-biblioref&#34;&gt;Zhao 2017&lt;/a&gt;)&lt;/span&gt;. This package provides many templates for a couple of academic works and hobbies. According to the help file, there are &lt;code&gt;academic article&lt;/code&gt;, &lt;code&gt;book&lt;/code&gt;, &lt;code&gt;thesis&lt;/code&gt;, &lt;code&gt;poster&lt;/code&gt;, and &lt;code&gt;journals&lt;/code&gt;. Because of Zhao’s interests, the users could write and publish &lt;code&gt;poem book&lt;/code&gt; and &lt;code&gt;guitar chords&lt;/code&gt;. The following pictures are the outcomes of these templates:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pzhaonet/bookdownplus/master/inst2/showcase/bookdownplus_article.jpg&#34; title=&#34;academic article&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pzhaonet/bookdownplus/master/inst2/showcase/bookdownplus_yihui_zh.jpg&#34; title=&#34;book&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pzhaonet/bookdownplus/master/inst2/showcase/bookdownplus_thesis_classic.jpg&#34; title=&#34;thesis&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pzhaonet/bookdownplus/master/inst2/showcase/bookdownplus_poem.jpg&#34; title=&#34;poster&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pzhaonet/bookdownplus/master/inst2/showcase/bookdownplus_journal.jpg&#34; title=&#34;journal&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pzhaonet/bookdownplus/master/inst2/showcase/bookdownplus_poster.jpg&#34; title=&#34;poem book&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pzhaonet/bookdownplus/master/inst2/showcase/bookdownplus_guitar.jpg&#34; title=&#34;guitar&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I believe ‘journal’ will be the hottest templaters for many research teams who are running their open practices. All the members of a project will have managed their daily journals collaboratively. The journal could be opened to public after the final paper is formly published.&lt;/p&gt;
&lt;p&gt;For the users who read and write traditional Chiense like me, you have to install some font files after you completed the install work flow as the mannual. According to my tests in WINDOWS 10, The font files are &lt;code&gt;KaiTi.ttf&lt;/code&gt;, &lt;code&gt;simfang.ttf&lt;/code&gt;, &lt;code&gt;simhei.ttf&lt;/code&gt;, and &lt;code&gt;simkai.ttf&lt;/code&gt;. Then you can use the Chinese templates in &lt;code&gt;bookdownplus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In future, I wish &lt;code&gt;bookdownplus&lt;/code&gt; could enhance the support for many multi-bytes language systems. The users could provide and share the customed templates through the updates. This will be the handy tool for the open scientists out of WIERD world.&lt;/p&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-AustpapajaReproducibleAPA2017&#34; class=&#34;csl-entry&#34;&gt;
Aust, Frederik, and Marius Barth. 2017. &lt;em&gt;Papaja: &lt;span&gt;Reproducible APA&lt;/span&gt; Manuscripts with &lt;span&gt;R Markdown&lt;/span&gt;&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-RDevelopmentCoreTeamlanguageenvironmentstatistical2010&#34; class=&#34;csl-entry&#34;&gt;
R Development Core Team. 2010. &lt;span&gt;“R: &lt;span&gt;A&lt;/span&gt; Language and Environment for Statistical Computing.”&lt;/span&gt; Vienna, Austria: &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-XiebookdownAuthoringBooks2017&#34; class=&#34;csl-entry&#34;&gt;
Xie, Yihui. 2017. &lt;em&gt;Bookdown: &lt;span&gt;Authoring Books&lt;/span&gt; and &lt;span&gt;Technical Documents&lt;/span&gt; with &lt;span&gt;R Markdown&lt;/span&gt;&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-ZhaobookdownplusTextbook2017&#34; class=&#34;csl-entry&#34;&gt;
Zhao, Peng. 2017. &lt;em&gt;R Bookdownplus &lt;span&gt;Textbook&lt;/span&gt;&lt;/em&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Manage a blog for the bibliography management optimization</title>
      <link>https://scchen.com/post/2017-05-08-bibliography-management-optimization/</link>
      <pubDate>Mon, 08 May 2017 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2017-05-08-bibliography-management-optimization/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;ul&gt;
&lt;li&gt;Because of my limited experiences, the recommendations might be useless for the users of EndNote, Mendeley, and the other reference management software. Although the concrete suggesions for the bloggers on many blog services are unavailable, the special recommendations and experiences are welcome.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many promoters of open-accessible literature spread their preprinted articles through the personal and collaborative blogs. Bloggers and followers like me usually manage the last updated posts in the reference managers. My &lt;a href=&#34;https://www.zotero.org/&#34;&gt;zotero&lt;/a&gt; have collected a lot of blog posts to be commented and to be cited in my coming articles. After collected hundreds of posts, I found that zotero not always imported the meta data of the blog posts for the correct citations. This post presents why not all the necessary meta data can be imported by the reference managers, and shows how the readers and bloggers could manage these situations.&lt;/p&gt;
&lt;p&gt;Referring to the 76th example of APA 6th style and the &lt;a href=&#34;http://blog.apastyle.org/apastyle/2016/04/how-to-cite-a-blog-post-in-apa-style.html&#34;&gt;explain of Timothy McAdoo (2016)&lt;/a&gt;, I demonstrates the reference format of a blog post like following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Who&amp;quot;. (Year, Month, Day). &amp;quot;Title of the post&amp;quot;[Blog post]. Retrieved from &amp;quot;my url&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a long time, I had been confused why some imported posts have no released date, and some do not include the author’s name. Until read this blog post about &lt;a href=&#34;https://moz.com/blog/meta-data-templates-123&#34;&gt;social meta tags&lt;/a&gt;, I realized that &lt;strong&gt;zotero&lt;/strong&gt; establish the items for the blog posts and webpages in terms of the &lt;code&gt;meta&lt;/code&gt; tags in the source html files. To have the correct citation of a blog post, a post at least has to have the three &lt;code&gt;meta&lt;/code&gt; tags: &lt;code&gt;author&lt;/code&gt;, &lt;code&gt;article:published_time&lt;/code&gt;, &lt;code&gt;og:title&lt;/code&gt;. I built a &lt;a href=&#34;http://scchen.com/blog/2017/05/templete-of-a-citable-blog-post.html&#34;&gt;template&lt;/a&gt; showing how these tag are included in my &lt;code&gt;head.html&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Many of my collected posts are from the active psychologists and statisticans. &lt;a href=&#34;http://psychbrief.com&#34;&gt;PsychBrief&lt;/a&gt; has gathered the &lt;a href=&#34;http://psychbrief.com/psychological-methods-blog-feed/&#34;&gt;43 this kind of academic blogs&lt;/a&gt;. After a short survey, I found that &lt;strong&gt;WordPress&lt;/strong&gt; is the favorite of these bloggers (29 blogs). &lt;strong&gt;Google blogpost&lt;/strong&gt; is the second preference (7 blogs). Almost all the &lt;strong&gt;WordPress&lt;/strong&gt; blog posts are able to export the &lt;code&gt;meta&lt;/code&gt; tags for the correct citations. However, some collaborative blogs, such as Andrew Gelman’s &lt;a href=&#34;http://andrewgelman.com/&#34;&gt;Statistical Modeling, Causal Inference, and Social Science&lt;/a&gt;, have no the &lt;code&gt;author&lt;/code&gt; tag. &lt;strong&gt;Google blogpost&lt;/strong&gt; at default have no the tag &lt;code&gt;article:published_time&lt;/code&gt;. As a user of &lt;strong&gt;zotero&lt;/strong&gt;, I have to fill in these information by hands after imported these blog posts.&lt;/p&gt;
&lt;p&gt;To help the supporters of open accessible literature have the efficient bibliography management, I summarize three situations according to my survey and experience:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Single bloggers manages the &lt;strong&gt;WordPress&lt;/strong&gt; blog.&lt;br /&gt;
&lt;em&gt;For readers&lt;/em&gt;: It’s fortune. All necessary tags are able to be imported.&lt;br /&gt;
&lt;em&gt;For bloggers&lt;/em&gt;: The author’s name have been registered since the day you opened your blog. When some posts are collaborative works, the default &lt;code&gt;author&lt;/code&gt; tag many not show the authorship as your wish.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Many bloggers share one &lt;strong&gt;WordPress&lt;/strong&gt; blog; Bloggers manage the personal site in the other services.&lt;br /&gt;
&lt;em&gt;For readers&lt;/em&gt;: Keep in mind that not all the necessary tags could be exported from these posts.&lt;br /&gt;
&lt;em&gt;For bloogers&lt;/em&gt;: If you want to help your readers’ bibliography managements easier, you could take hours to study how to put all the necessary &lt;code&gt;meta&lt;/code&gt; tags on each post.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bloggers manage the &lt;strong&gt;jekyll-based&lt;/strong&gt; blog.&lt;br /&gt;
&lt;em&gt;For readers&lt;/em&gt;: It’s the worst that many jekyll themes set the fewest &lt;code&gt;meta&lt;/code&gt; tags at default. On the ohter hand, it’s the best that the bloggers realized every things the readers need.&lt;br /&gt;
&lt;em&gt;For bloggers&lt;/em&gt;: In use of the &lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&#34;&gt;markdown syntax&lt;/a&gt;, your blog post is able to fetch the &lt;code&gt;author&lt;/code&gt;, &lt;code&gt;published_time&lt;/code&gt;, and &lt;code&gt;og:title&lt;/code&gt; in &lt;a href=&#34;https://en.wikipedia.org/wiki/YAML&#34;&gt;YAML&lt;/a&gt;. There is the flexibility that a blog collects the works of a single author and the collaborative consequence.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Manage a blog for the bibliography management optimization</title>
      <link>https://scchen.com/post/2017-05-08-manage-a-blog-for-the-easier-bibliography-management-optimization/</link>
      <pubDate>Mon, 08 May 2017 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2017-05-08-manage-a-blog-for-the-easier-bibliography-management-optimization/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;ul&gt;
&lt;li&gt;Because of my limited experiences, the recommendations might be useless for the users of EndNote, Mendeley, and the other reference management software. Although the concrete suggesions for the bloggers on many blog services are unavailable, the special recommendations and experiences are welcome.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many promoters of open-accessible literature spread their preprinted articles through the personal and collaborative blogs. Bloggers and followers like me usually manage the last updated posts in the reference managers. My &lt;a href=&#34;https://www.zotero.org/&#34;&gt;zotero&lt;/a&gt; have collected a lot of blog posts to be commented and to be cited in my coming articles. After collected hundreds of posts, I found that zotero not always imported the meta data of the blog posts for the correct citations. This post presents why not all the necessary meta data can be imported by the reference managers, and shows how the readers and bloggers could manage these situations.&lt;/p&gt;
&lt;p&gt;Referring to the 76th example of APA 6th style and the &lt;a href=&#34;http://blog.apastyle.org/apastyle/2016/04/how-to-cite-a-blog-post-in-apa-style.html&#34;&gt;explain of Timothy McAdoo (2016)&lt;/a&gt;, I demonstrates the reference format of a blog post like following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Who&amp;quot;. (Year, Month, Day). &amp;quot;Title of the post&amp;quot;[Blog post]. Retrieved from &amp;quot;my url&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a long time, I had been confused why some imported posts have no released date, and some do not include the author’s name. Until read this blog post about &lt;a href=&#34;https://moz.com/blog/meta-data-templates-123&#34;&gt;social meta tags&lt;/a&gt;, I realized that &lt;strong&gt;zotero&lt;/strong&gt; establish the items for the blog posts and webpages in terms of the &lt;code&gt;meta&lt;/code&gt; tags in the source html files. To have the correct citation of a blog post, a post at least has to have the three &lt;code&gt;meta&lt;/code&gt; tags: &lt;code&gt;author&lt;/code&gt;, &lt;code&gt;article:published_time&lt;/code&gt;, &lt;code&gt;og:title&lt;/code&gt;. I built a &lt;a href=&#34;http://scchen.com/blog/2017/05/templete-of-a-citable-blog-post.html&#34;&gt;template&lt;/a&gt; showing how these tag are included in my &lt;code&gt;head.html&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Many of my collected posts are from the active psychologists and statisticans. &lt;a href=&#34;http://psychbrief.com&#34;&gt;PsychBrief&lt;/a&gt; has gathered the &lt;a href=&#34;http://psychbrief.com/psychological-methods-blog-feed/&#34;&gt;43 this kind of academic blogs&lt;/a&gt;. After a short survey, I found that &lt;strong&gt;WordPress&lt;/strong&gt; is the favorite of these bloggers (29 blogs). &lt;strong&gt;Google blogpost&lt;/strong&gt; is the second preference (7 blogs). Almost all the &lt;strong&gt;WordPress&lt;/strong&gt; blog posts are able to export the &lt;code&gt;meta&lt;/code&gt; tags for the correct citations. However, some collaborative blogs, such as Andrew Gelman’s &lt;a href=&#34;http://andrewgelman.com/&#34;&gt;Statistical Modeling, Causal Inference, and Social Science&lt;/a&gt;, have no the &lt;code&gt;author&lt;/code&gt; tag. &lt;strong&gt;Google blogpost&lt;/strong&gt; at default have no the tag &lt;code&gt;article:published_time&lt;/code&gt;. As a user of &lt;strong&gt;zotero&lt;/strong&gt;, I have to fill in these information by hands after imported these blog posts.&lt;/p&gt;
&lt;p&gt;To help the supporters of open accessible literature have the efficient bibliography management, I summarize three situations according to my survey and experience:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Single bloggers manages the &lt;strong&gt;WordPress&lt;/strong&gt; blog.&lt;br /&gt;
&lt;em&gt;For readers&lt;/em&gt;: It’s fortune. All necessary tags are able to be imported.&lt;br /&gt;
&lt;em&gt;For bloggers&lt;/em&gt;: The author’s name have been registered since the day you opened your blog. When some posts are collaborative works, the default &lt;code&gt;author&lt;/code&gt; tag many not show the authorship as your wish.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Many bloggers share one &lt;strong&gt;WordPress&lt;/strong&gt; blog; Bloggers manage the personal site in the other services.&lt;br /&gt;
&lt;em&gt;For readers&lt;/em&gt;: Keep in mind that not all the necessary tags could be exported from these posts.&lt;br /&gt;
&lt;em&gt;For bloogers&lt;/em&gt;: If you want to help your readers’ bibliography managements easier, you could take hours to study how to put all the necessary &lt;code&gt;meta&lt;/code&gt; tags on each post.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bloggers manage the &lt;strong&gt;jekyll-based&lt;/strong&gt; blog.&lt;br /&gt;
&lt;em&gt;For readers&lt;/em&gt;: It’s the worst that many jekyll themes set the fewest &lt;code&gt;meta&lt;/code&gt; tags at default. On the ohter hand, it’s the best that the bloggers realized every things the readers need.&lt;br /&gt;
&lt;em&gt;For bloggers&lt;/em&gt;: In use of the &lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&#34;&gt;markdown syntax&lt;/a&gt;, your blog post is able to fetch the &lt;code&gt;author&lt;/code&gt;, &lt;code&gt;published_time&lt;/code&gt;, and &lt;code&gt;og:title&lt;/code&gt; in &lt;a href=&#34;https://en.wikipedia.org/wiki/YAML&#34;&gt;YAML&lt;/a&gt;. There is the flexibility that a blog collects the works of a single author and the collaborative consequence.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Template of a citable blog post</title>
      <link>https://scchen.com/post/2017-05-05-templete-of-a-citable-blog-post/</link>
      <pubDate>Fri, 05 May 2017 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2017-05-05-templete-of-a-citable-blog-post/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;this-post-is-for-future-posts.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;This post is for future posts.&lt;/h2&gt;
&lt;p&gt;Check the lines 5 to 9 of my post template &lt;a href=&#34;https://github.com/SCgeeker/SCgeeker.github.io/blob/master/_includes/head.html&#34;&gt;&lt;code&gt;head.html&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;html&#34;&gt;&lt;code&gt;    &amp;lt;meta name=&amp;quot;generator&amp;quot; content=&amp;quot;blogger&amp;quot;/&amp;gt;
    &amp;lt;meta name=&amp;quot;author&amp;quot; content=&amp;quot;Sau-Chin Chen&amp;quot;/&amp;gt;
    # A if-then loop imports page.title.
    &amp;lt;meta property=&amp;quot;og:title&amp;quot; content=&amp;quot;{% if page.title %}{{ page.title }}{% else %}{{ site.title }}{% endif %}&amp;quot;/&amp;gt;
    # A if-then loop imports page.date.
    &amp;lt;meta property=&amp;quot;article:published_time&amp;quot; content=&amp;quot;{% if page.date %}{{ page.date | date: &amp;quot;%B %-d, %Y&amp;quot;}}{% else %}{{ site.date }}{% endif %}&amp;quot;/&amp;gt;
    &amp;lt;meta property=&amp;quot;og:site_name&amp;quot; content=&amp;quot;Sau-Chin Chen&amp;#39;s website&amp;quot;/&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Plain Markdown Post</title>
      <link>https://scchen.com/post/2016-12-30-hello-markdown/</link>
      <pubDate>Fri, 30 Dec 2016 21:49:57 -0700</pubDate>
      <guid>https://scchen.com/post/2016-12-30-hello-markdown/</guid>
      <description>&lt;p&gt;This is a post written in plain Markdown (&lt;code&gt;*.md&lt;/code&gt;) instead of R Markdown (&lt;code&gt;*.Rmd&lt;/code&gt;). The major differences are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You cannot run any R code in a plain Markdown document, whereas in an R Markdown document, you can embed R code chunks (&lt;code&gt;```{r}&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;A plain Markdown post is rendered through &lt;a href=&#34;https://gohugo.io/overview/configuration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blackfriday&lt;/a&gt;, and an R Markdown document is compiled by &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;rmarkdown&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;http://pandoc.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pandoc&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are many differences in syntax between Blackfriday&amp;rsquo;s Markdown and Pandoc&amp;rsquo;s Markdown. For example, you can write a task list with Blackfriday but not with Pandoc:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write an R package.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write a book.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Profit!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similarly, Blackfriday does not support LaTeX math and Pandoc does. I have added the MathJax support to this theme (&lt;a href=&#34;https://github.com/yihui/hugo-lithium&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hugo-lithium&lt;/a&gt;) but there is a caveat for plain Markdown posts: you have to include math expressions in a pair of backticks (inline: &lt;code&gt;`$ $`&lt;/code&gt;; display style: &lt;code&gt;`$$ $$`&lt;/code&gt;), e.g., &lt;code&gt;$S_n = \sum_{i=1}^n X_i$&lt;/code&gt;.^[This is because we have to protect the math expressions from being interpreted as Markdown. You may not need the backticks if your math expression does not contain any special Markdown syntax such as underscores or asterisks, but it is always a safer choice to use backticks. When you happen to have a pair of literal dollar signs inside the same element, you can escape one dollar sign, e.g., &lt;code&gt;\$50 and $100&lt;/code&gt; renders &amp;ldquo;$50 and $100&amp;rdquo;.] For R Markdown posts, you do not need the backticks, because Pandoc can identify and process math expressions.&lt;/p&gt;
&lt;p&gt;When creating a new post, you have to decide whether the post format is Markdown or R Markdown, and this can be done via the &lt;code&gt;ext&lt;/code&gt; argument of the function &lt;code&gt;blogdown::new_post()&lt;/code&gt;, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::new_post(&amp;quot;Post Title&amp;quot;, ext = &#39;.Rmd&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Randomization in Latin Square</title>
      <link>https://scchen.com/post/2016-09-14-randomization-in-latin-square/</link>
      <pubDate>Wed, 14 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-09-14-randomization-in-latin-square/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’m managing an opensesame script for the coming project. In this project, there are four stimuli lists included all the within-participant conditions. I denote the four lists &lt;strong&gt;A&lt;/strong&gt;, &lt;strong&gt;B&lt;/strong&gt;, &lt;strong&gt;C&lt;/strong&gt;, and &lt;strong&gt;D&lt;/strong&gt;. A participant will have one of the stimuli lists in terms of the counter balanced principle. The best assignment method is 4X4 Latin Square. For every 16 participants, there are 24 sequences to be used. Here are three of the sequences.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,] &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot; 
## [2,] &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot; 
## [3,] &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot; 
## [4,] &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,] &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot; 
## [2,] &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot; 
## [3,] &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot; 
## [4,] &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,] &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot; 
## [2,] &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot; 
## [3,] &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot; 
## [4,] &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;  &amp;quot;D&amp;quot;  &amp;quot;A&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If this experiment will recruit 32 participant, the first 16 and second 16 better obtain one of the 24 sequences in a pseudo randomization. On the other hand, I wish to reproduce this random sequence in the other place. The best way is to generate the randomization sequence in terms of a seed. Thanks to &lt;a href=&#34;http://osdoc.cogsci.nl/&#34;&gt;Opensesame&lt;/a&gt;, I found the way to make this plan come true.&lt;/p&gt;
&lt;p&gt;Opensesame is the open source experiment software. Because it is developed in &lt;a href=&#34;https://www.python.org/&#34;&gt;python&lt;/a&gt;, the users are able to create the function based on their need. This program offers the object &lt;strong&gt;inline_script&lt;/strong&gt; where the users put the python codes. I created the python codes to select the stimuli list from the file pool. You can use it before the program runs the object &lt;strong&gt;loop&lt;/strong&gt; based on the list.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;### Get the subject number
nr = self.get(&amp;#39;subject_nr&amp;#39;)

### Generate a random sequence based the prior seed
### Defind seed by yourself
import random
SEED = 345

seq = [0,4,8,12]
random.seed(SEED)
random.shuffle(seq)

### Shift the subject numbers
### Push to the next sequence
if nr &amp;lt;=16:
    nr += seq[0]
elif nr &amp;gt; 16 &amp;amp; nr &amp;lt;= 32:
    nr += seq[1]
elif nr &amp;gt; 32 &amp;amp; nr &amp;lt;= 48:
    nr += seq[2]
else:
    nr += seq[3]

### Assign the list in one Latin Squane
if (nr / 4) % 4 == 1:
    if nr % 4 == 0:
        lst = &amp;#39;List01.csv&amp;#39;
    elif nr % 4 == 1:
        lst = &amp;#39;List02.csv&amp;#39;
    elif nr % 4 == 2:
        lst = &amp;#39;List03.csv&amp;#39;
    else:
        lst = &amp;#39;List04.csv&amp;#39;
elif (nr / 4) % 4 == 2:
    if nr % 4 == 1:
        lst = &amp;#39;List01.csv&amp;#39;
    elif nr % 4 == 2:
        lst = &amp;#39;List02.csv&amp;#39;
    elif nr % 4 == 3:
        lst = &amp;#39;List03.csv&amp;#39;
    else:
        lst = &amp;#39;List04.csv&amp;#39;
elif (nr / 4) % 4 == 3:
    if nr % 4 == 2:
        lst = &amp;#39;List01.csv&amp;#39;
    elif nr % 4 == 3:
        lst = &amp;#39;List02.csv&amp;#39;
    elif nr % 4 == 0:
        lst = &amp;#39;List03.csv&amp;#39;
    else:
        lst = &amp;#39;List04.csv&amp;#39;
else:
    if nr % 4 == 3:
        lst = &amp;#39;List01.csv&amp;#39;
    elif nr % 4 == 0:
        lst = &amp;#39;List02.csv&amp;#39;
    elif nr % 4 == 1:
        lst = &amp;#39;List03.csv&amp;#39;
    else:
        lst = &amp;#39;List04.csv&amp;#39;

### Output [List] to the loop object
exp.set(&amp;#39;List&amp;#39;, lst)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Why we need a readable data table in behavioral scientific research?</title>
      <link>https://scchen.com/post/2016-05-31-why-we-need-a-readable-data-table-in-behavioral-scientific-research/</link>
      <pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-05-31-why-we-need-a-readable-data-table-in-behavioral-scientific-research/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;a-bad-case&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A bad case&lt;/h3&gt;
&lt;p&gt;A few days ago I was dealing with a raw data set as like the following table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;ID&lt;/th&gt;
&lt;th&gt;Group&lt;/th&gt;
&lt;th&gt;I01&lt;/th&gt;
&lt;th&gt;I02&lt;/th&gt;
&lt;th&gt;I03&lt;/th&gt;
&lt;th&gt;I04&lt;/th&gt;
&lt;th&gt;I05&lt;/th&gt;
&lt;th&gt;I06&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;s01&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;s02&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;s03&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;s04&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;s05&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;s06&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This table is a tiny version I copied from a master graduate’s spss raw data table. The format of the table is frequently seen in the data files for SPSS software. This format originated the “row cognition” for data in every mind who are not familar with the statistical tools. Without the awareness of structure, people used to stack a lot of bricks horizontally. As like the table shows, there are the data of 6 participants. A new statistical tool user tend to fill all the information and data for a participant in a row as possible. This format fits human cognition for the naive thing who never manipulate before. However, the computers recognize the table by column, in other words, a “column cognition”. I show you how computer deal with the summation of one participant’s scores by the following codes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s01_data &amp;lt;- c(3,4,5,1,2,3)
units &amp;lt;- rep(1,6)
sum(s01_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] 18&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t(units)%*%s01_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [,1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1,] 18&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;sum&lt;/code&gt; is one of the first functions a new statistical tool user have learned at the first class. &lt;code&gt;sum&lt;/code&gt; is used to add all the scores of a participant. A new user tend to append the summation to the end of a row. But the last code show you a programing language recognizes the scores of a participant by column. The evidence is that the sturcture of &lt;code&gt;t(units)&lt;/code&gt; is row but the structure of &lt;code&gt;s01_data&lt;/code&gt; is a column. How do we eliminate the difference between human and machine cognition? Why human tend to deal with the data by row rather by column?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;codes-to-make-the-table-readable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Codes to make the table readable&lt;/h3&gt;
&lt;p&gt;To make the table readable for the computer, I have to modify the format of this table as like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Table &amp;lt;- read.csv(&amp;quot;data.csv&amp;quot;)
New_Table = data.frame(ID = rep(Table[,1],6), Group = rep(Table[,2],6), Item = rep(names(Table[,3:8]), each = 6), Resp = unlist(c(Table[,3:8])), row.names = NULL)
head(New_Table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ID Group Item Resp
## 1 s01     A  I01    3
## 2 s02     A  I01    2
## 3 s03     A  I01    4
## 4 s04     B  I01    2
## 5 s05     B  I01    2
## 6 s06     B  I01    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new table arrange all the raw scores in one column. I create a new column &lt;code&gt;Item&lt;/code&gt; in this table. &lt;code&gt;Item&lt;/code&gt; is the index like &lt;code&gt;ID&lt;/code&gt; and &lt;code&gt;Group&lt;/code&gt;. We are able to summarize the total score of each participant by combine the index ‘ID’ and ‘Item’. We also average the group means by the index &lt;code&gt;Group&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;configurate-human-mind-approaching-a-readable-data-table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Configurate human mind approaching a readable data table&lt;/h3&gt;
&lt;p&gt;Why we have to make effort for waving a table readable for the computer? It is the human nature we prefer every column filled with meanings in a table. Every item has the reason a researcher put it into the study. Organizing all scores of items in a row occupy the meanings in the human cognition, but the index of items is meaningless for every human mind. A researcher need a table filled with the analyzable data for the statitical tool. In the case of this post, the analyzable data is the summation of all 6 items. A table like this case embodies the desired table for the researcher but needs a column of summarized scores for the computer. This gap has to be fixed before a precise statistical process is conducted. The solution I prefer is design the table format at first. This is the main reason I and my students have to prepare a code book before the start of the experiment.&lt;/p&gt;
&lt;p&gt;The secondary solution depends on the packaged functions. There are many raw data table organized like the bad case at first, because the software to collect the raw data and organize them in this way, for example, google From apps. To deal with these data, I have to transform the raw data before I import them to R. A reproducible way is to transform them in use of the R codes. The functions have to isolate which columns are index and which columns are raw responses. To my knowledge, a code book is needed to define the columns to be arranged.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why we need a readable data table in behavioral scientific research?</title>
      <link>https://scchen.com/post/2016-05-31-why-we-need-a-readable-data-table/</link>
      <pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-05-31-why-we-need-a-readable-data-table/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;a-bad-case&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A bad case&lt;/h3&gt;
&lt;p&gt;A few days ago I was dealing with a raw data set as like the following table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;ID&lt;/th&gt;
&lt;th&gt;Group&lt;/th&gt;
&lt;th&gt;I01&lt;/th&gt;
&lt;th&gt;I02&lt;/th&gt;
&lt;th&gt;I03&lt;/th&gt;
&lt;th&gt;I04&lt;/th&gt;
&lt;th&gt;I05&lt;/th&gt;
&lt;th&gt;I06&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;s01&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;s02&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;s03&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;s04&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;s05&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;s06&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This table is a tiny version I copied from a master graduate’s spss raw data table. The format of the table is frequently seen in the data files for SPSS software. This format originated the “row cognition” for data in every mind who are not familar with the statistical tools. Without the awareness of structure, people used to stack a lot of bricks horizontally. As like the table shows, there are the data of 6 participants. A new statistical tool user tend to fill all the information and data for a participant in a row as possible. This format fits human cognition for the naive thing who never manipulate before. However, the computers recognize the table by column, in other words, a “column cognition”. I show you how computer deal with the summation of one participant’s scores by the following codes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s01_data &amp;lt;- c(3,4,5,1,2,3)
units &amp;lt;- rep(1,6)
sum(s01_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] 18&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t(units)%*%s01_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [,1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1,] 18&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;sum&lt;/code&gt; is one of the first functions a new statistical tool user have learned at the first class. &lt;code&gt;sum&lt;/code&gt; is used to add all the scores of a participant. A new user tend to append the summation to the end of a row. But the last code show you a programing language recognizes the scores of a participant by column. The evidence is that the sturcture of &lt;code&gt;t(units)&lt;/code&gt; is row but the structure of &lt;code&gt;s01_data&lt;/code&gt; is a column. How do we eliminate the difference between human and machine cognition? Why human tend to deal with the data by row rather by column?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;codes-to-make-the-table-readable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Codes to make the table readable&lt;/h3&gt;
&lt;p&gt;To make the table readable for the computer, I have to modify the format of this table as like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Table &amp;lt;- read.csv(&amp;quot;data.csv&amp;quot;)
New_Table = data.frame(ID = rep(Table[,1],6), Group = rep(Table[,2],6), Item = rep(names(Table[,3:8]), each = 6), Resp = unlist(c(Table[,3:8])), row.names = NULL)
head(New_Table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ID Group Item Resp
## 1 s01     A  I01    3
## 2 s02     A  I01    2
## 3 s03     A  I01    4
## 4 s04     B  I01    2
## 5 s05     B  I01    2
## 6 s06     B  I01    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new table arrange all the raw scores in one column. I create a new column &lt;code&gt;Item&lt;/code&gt; in this table. &lt;code&gt;Item&lt;/code&gt; is the index like &lt;code&gt;ID&lt;/code&gt; and &lt;code&gt;Group&lt;/code&gt;. We are able to summarize the total score of each participant by combine the index ‘ID’ and ‘Item’. We also average the group means by the index &lt;code&gt;Group&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;configurate-human-mind-approaching-a-readable-data-table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Configurate human mind approaching a readable data table&lt;/h3&gt;
&lt;p&gt;Why we have to make effort for waving a table readable for the computer? It is the human nature we prefer every column filled with meanings in a table. Every item has the reason a researcher put it into the study. Organizing all scores of items in a row occupy the meanings in the human cognition, but the index of items is meaningless for every human mind. A researcher need a table filled with the analyzable data for the statitical tool. In the case of this post, the analyzable data is the summation of all 6 items. A table like this case embodies the desired table for the researcher but needs a column of summarized scores for the computer. This gap has to be fixed before a precise statistical process is conducted. The solution I prefer is design the table format at first. This is the main reason I and my students have to prepare a code book before the start of the experiment.&lt;/p&gt;
&lt;p&gt;The secondary solution depends on the packaged functions. There are many raw data table organized like the bad case at first, because the software to collect the raw data and organize them in this way, for example, google From apps. To deal with these data, I have to transform the raw data before I import them to R. A reproducible way is to transform them in use of the R codes. The functions have to isolate which columns are index and which columns are raw responses. To my knowledge, a code book is needed to define the columns to be arranged.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rethink Significance</title>
      <link>https://scchen.com/post/2016-05-05-rethink-significance/</link>
      <pubDate>Thu, 05 May 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-05-05-rethink-significance/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;To trace the fallacy of use hypothesis testing, I am programming the examples in Dr. Adrianus D. de Groot’s paper &lt;code&gt;The meaning of “signiﬁcance” for different types of research&lt;/code&gt;. This paper is published in 1956 in the Dutch journal &lt;em&gt;Nederlands Tijdschrift voor de Psychologie en Haar Grensgebieden&lt;/em&gt;. &lt;a href=&#34;https://en.wikipedia.org/wiki/Adriaan_de_Groot&#34;&gt;Adrianus De Groot&lt;/a&gt; is Dutch psychologist and chess master. During 1950s and 1960s, he suggested the concept of emperical cycle for the researchers who use the statistical tools in the social and behavioral researches.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://scchen.com/images/524px-Empirical_Cycle.png&#34; title=&#34;A visual representation of A.D. de Groot&amp;#39;s empirical cycle. Author: TesseUndDaan&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Emperical cycle&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;de Groot’s emperical cycle distinguish two types of emperical research: &lt;strong&gt;exploratory research&lt;/strong&gt; and &lt;strong&gt;confirmatory research&lt;/strong&gt;. &lt;strong&gt;Exploratory research&lt;/strong&gt; aims to formulate the hypotheses that covers the process from &lt;code&gt;observation&lt;/code&gt; to &lt;code&gt;induction&lt;/code&gt;. &lt;strong&gt;Confirmatory research&lt;/strong&gt; attempts to test the predicited concequences based the hypothesis. Thus a &lt;strong&gt;Confirmatory research&lt;/strong&gt; covers &lt;code&gt;deduction&lt;/code&gt;, &lt;code&gt;testing&lt;/code&gt;, and &lt;code&gt;evaluation&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the time de Groot started his academic career, the protocol of hypothesis testing has been completed by the leading statisticans, &lt;a href=&#34;https://en.wikipedia.org/wiki/Ronald_Fisher&#34;&gt;Ronald Fisher&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Jerzy_Neyman&#34;&gt;Jerzy Neyman&lt;/a&gt;, and &lt;a href=&#34;https://en.wikipedia.org/wiki/Egon_Pearson&#34;&gt;Egon Pearson&lt;/a&gt;. Psychologists at that time learned and used &lt;em&gt;p value&lt;/em&gt; to evaluate the results. de Groot in his paper has suggested that the hypothesis testing is the appropriate tool to evaluate the result of a confirmatory reserach. He in the same paper also discussed the problems that perhapes happen when the hypothesis testing was used to decide the available hypotheses in an exploratory research. There are two cases of explorartoy research discussed in his paper. One has finite hypotheses, and the other has infinite hypotheses. Both cases show up in front of every researcher in anytime and in anywhere. Many researcher struggle how to pick the available hypotheses up according to the data in hands.&lt;/p&gt;
&lt;div id=&#34;exploratory-research-has-finite-hypotheses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory research has finite hypotheses&lt;/h3&gt;
&lt;p&gt;The section title in de Groot’s paper is &lt;code&gt;Hypothesis testing research for multiple hypotheses&lt;/code&gt;. He presumed the case as follow:&lt;/p&gt;
&lt;p&gt;We give &lt;em&gt;N&lt;/em&gt; as the number of hypotheses yet to be tested. Every hypothesis is going to be evaluated by the hypothesis testing. We also give &lt;em&gt;n&lt;/em&gt; as the number of hypothese successfully passed the test. Every hypothesis have the probability .05 pass the test. This probability refers to the type 1 error in the present hypothesis testing.&lt;/p&gt;
&lt;p&gt;Today we have 10 hypotheses (N = 10) to be evaluated by the data. Consider all the consequences, we can calculate all the probabilities given the counts of positive hypotheses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(xtable)
N &amp;lt;- 10
n &amp;lt;- 0:10
alpha &amp;lt;- .05
REJECT_P &amp;lt;- 0
PASS_P &amp;lt;- rep(0,length(n))
for(k in n){
        REJECT_P = REJECT_P + choose(N,k)*alpha^k*(1 - alpha)^(N - k) 
        PASS_P[k+1] = 1 - REJECT_P
}
SUCCESS = data.frame(n,PASS_P)
colnames(SUCCESS) &amp;lt;- c(&amp;quot;n&amp;quot;, &amp;quot;Probability(Positive Results)&amp;quot;)
print(xtable(SUCCESS), include.rownames = FALSE, type = &amp;quot;html&amp;quot; ) &lt;/code&gt;&lt;/pre&gt;
&lt;!-- html table generated in R 4.0.3 by xtable 1.8-4 package --&gt;
&lt;!-- Tue Jan 19 17:55:26 2021 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
n
&lt;/th&gt;
&lt;th&gt;
Probability(Positive Results)
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
0
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.09
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(PASS_P ~ n, xlab = &amp;quot;Smallest Number of Positive Hypotheses&amp;quot;, ylab = &amp;quot;Probability(Positive Results)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://scchen.com/en/post/2016-05-05-rethink-significance_files/figure-html/MultiHypo-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That table is telling us if we wish acquire at least one positive hypothesis, the probability is 0.4012631. That plot shows the probability dramatically decrease with the increasing numbers in our wish. In other words, when we have no precise knowlegde about the use of hyphtesis testing, the more hypotheses we want to induce, the higher risk we get nothing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-research-has-infinite-hypotheses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory research has infinite hypotheses&lt;/h3&gt;
&lt;p&gt;This case is everywhere in this era of big data. de Groot called this case &lt;code&gt;Material-exploration: N becomes unspeciﬁed&lt;/code&gt;. In his paper, material refers to data because data is a low frequency word in his era. The researchers in his era has realized the best way to deal with this case is &lt;strong&gt;to let the data tell the story&lt;/strong&gt;. The research on this case obviously is an &lt;strong&gt;exploratory research&lt;/strong&gt;. For the researchers in de Groot’s era and in the era of big data, there are two routes to finish this kind of research project. One route is to label and categorize the hypotheses. This route is the hot topic of &lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt; in the present data science. The other route is to decide the possible hypotheses. Today the researchers on this route mostly rely on the multiple variate statistical tools.&lt;/p&gt;
&lt;p&gt;de Groot suggests the caution to the research that attempts to decide the potential hypotheses. &lt;strong&gt;N&lt;/strong&gt; is infinite because not all hypotheses could be induced in this case. Whe we have 20 testable hypotheses from 200 potential hypotheses. If a researcher confirmed that 10 of the 20 hypotheses have the positive support by the data, based on the type 1 error, he/she has to understand that 5% hypotheses are positive (10/200) but 50% hypotheses are positive (10/20). This caution is like the misuse of golem that is discussed in &lt;a href=&#34;https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-Stan/McElreath/9781482253443&#34;&gt;Statistical Rethingking&lt;/a&gt;. In the timing I am writing this note, this caution implied the researchers who are thinking the potential hypotheses are decided assumed that they have finished a emperical circle.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;role-of-preregisteration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Role of Preregisteration&lt;/h3&gt;
&lt;p&gt;Decades of misusing the hypothesis testing have resulted in a bad situation we have to faced. There is no clear cutoff between the exploratory research and the confirmatory research. Many published papers in nature are the exploratory researches, but they are packaged in the form of confirmatory research by the authors (editors and reviewers have the responsibility too). This is why de Groot’s originated paper was translated and published 60 years later. This post is one of the fundemental I will cite when I introduce and discuss the preregistration.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Autopsy of NHST and Bayesian Models (part 1)</title>
      <link>https://scchen.com/post/2016-04-30-autopsy-of-nhst-and-bayesian-models/</link>
      <pubDate>Sat, 30 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-04-30-autopsy-of-nhst-and-bayesian-models/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Chapter 2 of &lt;a href=&#34;http://xcelab.net/rm/statistical-rethinking/&#34;&gt;Statistical Rethinking: A Bayesian Course with Examples in R and Stan&lt;/a&gt; introduces a water tossing example for the demonstration how Bayesain model run through the data based on the researcher’s hypothesis. This chapter defines a three stage process that are used in the coming chapters. We start from a narrated &lt;strong&gt;Data Story&lt;/strong&gt;, then &lt;strong&gt;update&lt;/strong&gt; the models by filling data in, and finally &lt;strong&gt;evaluate&lt;/strong&gt; all the &lt;strong&gt;upated&lt;/strong&gt; models. This literated process shows a clear picture for the learners who have yet stuied statistics before read this book. The readers who have studied statistics, like me, will have a hole in the mind what are the differences between Bayesian methods and the null hypothesis significance testing (NHST) in this process. After read Michael Clark’s &lt;a href=&#34;https://sites.google.com/a/umich.edu/micl/miscfiles/IntroBayes.pdf&#34;&gt;Bayesian Basics: A Conceptual Introduction with Application in R and Stan&lt;/a&gt; on &lt;a href=&#34;http://mc-stan.org/documentation/&#34;&gt;Stan official site&lt;/a&gt;, I require the key to understand the difference between the two golems.&lt;/p&gt;
&lt;p&gt;The critical difference is which type of &lt;strong&gt;conditional probability&lt;/strong&gt; the statistical method is used to evaluate the model. Once we collected the data based on some hypothesis, we have the distribution of hypothesis $ p( ) $ and the distribution of data $ p(y) $. NHST compuates the probability we have the data when the hypothesis is true $ p(y|) $. In practical, &lt;em&gt;p&lt;/em&gt; value refers to the &lt;strong&gt;conditional probability&lt;/strong&gt; of the null hypothesis, and * confidence interval* suggests a range of plausible outcomes based on the &lt;em&gt;confiditional probability&lt;/em&gt; of the real hypothesis. A Bayesian method compuates the probability the hypothesis is approved based on the data we have $ p(|y) $. Because the computation of $ p(|y) $ needs the likelihood $ p(y|) $ and the priori $ p() $, of course a Baysian method cost more steps than NHST.&lt;/p&gt;
&lt;p&gt;$ $ has a term &lt;strong&gt;parameter&lt;/strong&gt; in many statstical textbooks. It is a space of numbers that provides the axis where the above probability distribution sit on. &lt;strong&gt;Likelihood&lt;/strong&gt; is the &lt;strong&gt;sampling distribution&lt;/strong&gt; we have to update before we run NHST. As like I show in &lt;a href=&#34;https://scchen.com/blog/2016/03/learning-sampling-distribution-in-r-programming.html&#34;&gt;Learning Sampling Distribution in R Programming&lt;/a&gt;, a sampling distribution will approach one $ $ with the increasing sample size. Sample size is the key for NHST because it could change the evaluation criteron on the statistical model.&lt;/p&gt;
&lt;p&gt;Here is a pseudo experiment I want to know if a test is success based on the expect value smaller than 5 (Obersvations are 1, 2, 3, 4, 5, 6, 7, 8, 9, 10). I completed an experiment of 25 observations and an experiment 36 observations. In use of NHST with a critical value (p &amp;lt; .05), the criterion change with the sample size. The simulation&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
OBV &amp;lt;- 1:10
Dist25 &amp;lt;- NULL
Dist36 &amp;lt;- NULL
count = 100
while(count &amp;gt; 0){Dist25 &amp;lt;- c(Dist25,mean(sample(OBV, 25,replace = TRUE) ) ); count &amp;lt;- count - 1}
Dist25_Density &amp;lt;- data.frame(Theta = density(Dist25, kernel = &amp;quot;gaussian&amp;quot;)$x, Density = density(Dist25, kernel = &amp;quot;gaussian&amp;quot;)$y)
CL25 &amp;lt;- max(Dist25_Density$Theta[Dist25_Density$Theta &amp;lt; 5 &amp;amp; Dist25_Density$Density &amp;lt; .05])
print(CL25)  ## The smallest parameter that could make judgement&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.286263&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count = 100
while(count &amp;gt; 0){Dist36 &amp;lt;- c(Dist36,mean(sample(OBV, 36,replace = TRUE) ) ); count &amp;lt;- count - 1}
Dist36_Density &amp;lt;- data.frame(Theta = density(Dist36, kernel = &amp;quot;gaussian&amp;quot;)$x, Density = density(Dist36, kernel = &amp;quot;gaussian&amp;quot;)$y)
CL36 &amp;lt;- max(Dist36_Density$Theta[Dist36_Density$Theta &amp;lt; 5 &amp;amp; Dist36_Density$Density &amp;lt; .05])
print(CL36)  ## The smallest parameter that could make judgement&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.345472&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When this experiment outputs a avarage value between 4.15 and 4.42, would you like to collect the data more than 36 participants? This is the opportunity many researchers have to struggle in their study. Researchers who are educated as like me used to collect the observation till the &lt;em&gt;p&lt;/em&gt; value is smaller than .05. Most researchers used to stop the experiment till the 36th participant finished the experiment. However, this treatment lacks of the serious scientific thinking. If the hypothesis has set up the conditions to have the average, the sample size should be appointed before the start of experiment.&lt;/p&gt;
&lt;p&gt;The appointment of sample size could be loose when the possible average is uncertain. When the experiment is conducted at first time in the world, NHST without the appointment of sample size could draw the range of averages that might obey the hypothesis. On the other hand, the following studies better has the appointment of sample size. In this case, NHST has to accompany the other statistical evaluation, such as power, effect size, to find the appropriate sample size. Therefore, NHST will perform well when the researcher conducted the first study or have the preparation as well as the first study. This will be the key in my autopsy of Bayesian method.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Check My Tools to Create and Manipulate Golems</title>
      <link>https://scchen.com/post/2016-04-22-check-my-tools-to-create-and-manipulate-golems/</link>
      <pubDate>Fri, 22 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-04-22-check-my-tools-to-create-and-manipulate-golems/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Since two weeks before I published this post, I have started read the book &lt;a href=&#34;http://xcelab.net/rm/statistical-rethinking/&#34;&gt;Statistical Rethinking: A Bayesian Course with Examples in R and Stan&lt;/a&gt; written by &lt;a href=&#34;http://xcelab.net/rm/&#34;&gt;Richard McElreath&lt;/a&gt;. Richard is the evolutionary anthropologist at Max Planck Institute. He wrote this textbook for the PhD students who will use the Bayesian statistics in their research projects. Compared to the textbooks written by statisians and data scientistst, Richard’s book explain and demonstrate the statistical methods with examples instead of equations. His intention is to help who are not staticians but used to use statistics realize one fact: we rely on the statistical models as the representations of our answers rather than answer the questions by the raw data or naked truth. Many non-statisticians are used to find and learn what kind of methods or apps to deal with their data, but few are interested to know the models under the mentods and apps they are using. The trouble and danger is that they thought their jobs are done when the program printed the tables and figures but these outputs are from the statistical model is unable to answer their question. This situation is originated from many non-statisticans consider themselves the end-users of statistical models. Like any user of a packaged software, non-statisticans have no time to understand how the tools in their hands designed and conducted by statisticans.&lt;br /&gt;
Richard introduced the story of &lt;a href=&#34;https://en.wikipedia.org/wiki/Golem&#34;&gt;golem&lt;/a&gt; to raise the non-statsiticans’ attention to the troubles they had made and will make. A statistical model, like a golem, has the power beyond human to finish the work the human are unable to do, for example, trace the passengers’ track from the trillion of camera. Its power could be misused or out of control if we do not understand what is the root of its action. A user of ststistical method, no matter you are or are not stistician, have to keep the awareness of engineer when you are dealing with your data. Today everyone has many easier ways than a decade ago to keep the awareness of engineer. One advantage is that the learning curve for being an part-time hacker is getting smooth. Increasing R apps are opening many windows for who are want to outlook and modify the statistical models.&lt;br /&gt;
Since this post, every post listed in the category &lt;code&gt;Rethinking&lt;/code&gt; is one of the summaries and feedbacks to &lt;a href=&#34;https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-Stan/McElreath/9781482253443&#34;&gt;Statistical Rethinking: A Bayesian Course with Examples in R and Stan&lt;/a&gt;. At first I have to check my toolkits for create and manipulate the statistical models. They are R core and the packages. Years ago I have used to use the packages in my data processing. Now I show them for who start to use R after read this post.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(c(&amp;quot;rpart&amp;quot;,&amp;quot;chron&amp;quot;,&amp;quot;Hmisc&amp;quot;,&amp;quot;Design&amp;quot;,&amp;quot;Matrix&amp;quot;,&amp;quot;stringr&amp;quot;,&amp;quot;lme4&amp;quot;,&amp;quot;coda&amp;quot;,&amp;quot;e1071&amp;quot;,&amp;quot;zipfR&amp;quot;,&amp;quot;ape&amp;quot;,&amp;quot;languageR&amp;quot;,&amp;quot;multcomp&amp;quot;,&amp;quot;contrast&amp;quot;,&amp;quot;shiny&amp;quot;,&amp;quot;ggplot2&amp;quot;, &amp;quot;dplyr&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of the packages are learned from I participated in &lt;a href=&#34;https://www.coursera.org/specializations/jhu-data-science&#34;&gt;Cousera Data Science&lt;/a&gt;. Now I used to use &lt;code&gt;dplyr&lt;/code&gt; to process the raw data, and I am learning how to draw the figures I need in use of &lt;code&gt;ggplot2&lt;/code&gt;. When this post is published, I have updated my R to R version 4.0.3 (2020-10-10). Through &lt;a href=&#34;http://xcelab.net/rm/statistical-rethinking/&#34;&gt;the codes of Heuristic Andrew&lt;/a&gt;, here are my installed packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ip &amp;lt;- as.data.frame(installed.packages()[,c(1,3:4)])
rownames(ip) &amp;lt;- NULL
ip &amp;lt;- ip[is.na(ip$Priority),1:2,drop=FALSE]
print(ip, row.names=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   Package    Version
##                     abind      1.4-5
##                   acepack      1.4.1
##                      afex     0.28-1
##                ANOVApower      0.0.3
##                   anytime      0.3.9
##                       apa      0.3.3
##                       ape      5.4-1
##                       arm     1.11-2
##                   askpass        1.1
##                assertthat      0.2.1
##                 backports      1.2.1
##                 base64enc      0.1-3
##               BayesFactor 0.9.12-4.2
##                bayestestR      0.8.0
##                     bbmle   1.0.23.1
##                 bdsmatrix      1.3-4
##                        BH   1.75.0-0
##                    bibtex    0.4.2.3
##                     bindr      0.1.1
##                  bindrcpp      0.2.2
##                    binman      0.1.2
##                     binom      1.1-1
##                    bitops      1.0-6
##                      blob      1.2.1
##                  blogdown        1.0
##                  bookdown       0.21
##                      brew      1.0-6
##            bridgesampling      1.0-0
##                      brio      1.1.0
##               Brobdingnag      1.2-6
##                     broom      0.7.3
##               broom.mixed      0.2.6
##                broomExtra      4.1.0
##                   BWStest      0.2.2
##                        ca     0.71.1
##                     callr      3.5.1
##                       car     3.0-10
##                   carData      3.0-4
##                   caTools     1.18.1
##                     cdata      1.1.9
##                cellranger      1.1.0
##                 checkmate      2.0.0
##                  circlize     0.4.12
##                      citr      0.3.2
##                       cli      2.2.0
##                     clipr      0.7.1
##                clisymbols      1.2.0
##                      coda     0.19-4
##                      coin      1.3-1
##                colorspace      2.0-0
##              colourpicker      1.1.0
##                commonmark        1.7
##                compute.es      0.2-5
##                   conquer      1.0.2
##                  contfrac     1.1-12
##               correlation      0.5.0
##                  corrplot       0.84
##                      covr      3.5.1
##                   cowplot      1.1.1
##                     cpp11      0.2.5
##                    crayon      1.3.4
##               credentials      1.3.0
##                 crosstalk      1.1.1
##                      crul      1.0.0
##                   cubelyr      1.0.1
##                      curl        4.3
##                   dagitty      0.3-0
##                data.table     1.13.6
##                       DBI      1.1.0
##                    dbplyr      2.0.0
##                      desc      1.2.0
##                 DescTools    0.99.39
##                   deSolve       1.28
##                  devtools      2.3.2
##               DiceKriging      1.5.8
##                 dichromat      2.0-0
##                   diffobj      0.3.3
##                    digest     0.6.27
##                 dotCall64      1.0-0
##                     dplyr      1.0.2
##                        DT       0.17
##                 dunn.test      1.3.5
##                  dygraphs    1.1.1.6
##                     e1071      1.7-4
##                effectsize      0.4.1
##                   effsize      0.8.1
##                   ellipse      0.4.2
##                  ellipsis      0.3.1
##                  elliptic      1.4-0
##                   emmeans      1.5.3
##                       emo 0.0.0.9000
##                       EMT        1.1
##               equivalence      0.7.2
##              estimability        1.3
##                  evaluate       0.14
##                     Exact        2.1
##            exactRankTests     0.8-31
##                      expm    0.999-6
##                 extrafont       0.17
##               extrafontdb        1.0
##                        ez      4.4-0
##                     fansi      0.4.1
##                    farver      2.0.3
##                fastGHQuad        1.0
##                   fastmap      1.0.1
##                      faux    0.0.1.5
##                   fauxpas      0.5.0
##                    fields       11.6
##                  finalfit      1.0.2
##           fivethirtyeight      0.6.1
##             flexdashboard      0.5.2
##                 flextable      0.6.1
##                   forcats      0.5.0
##                   foreach      1.5.1
##                   Formula      1.2-4
##             formula.tools      1.7.1
##                        fs      1.5.0
##                     furrr      0.2.1
##                    future     1.21.0
##             gameofthrones      1.0.2
##                 gapminder      0.3.0
##                    gargle      0.5.0
##                     gdata     2.18.0
##                   gdtools      0.2.3
##                  generics      0.1.0
##                 geosphere     1.5-10
##                      gert      1.0.2
##                   getPass      0.2-2
##                ggalluvial     0.12.3
##                    GGally      2.1.0
##                ggcorrplot      0.1.3
##                 ggeffects      1.0.1
##                   ggExtra        0.9
##                   ggforce      0.3.2
##                       ggm        2.5
##                     ggmap      3.0.0
##                   ggplot2      3.3.3
##             ggplot2movies      0.0.1
##                    ggpubr      0.4.0
##                    ggraph      2.0.4
##                   ggrepel      0.9.0
##                  ggridges      0.5.3
##                     ggsci        2.9
##                  ggsignif      0.6.0
##               ggstatsplot      0.6.6
##                  ggthemes      4.2.0
##                        gh      1.2.0
##                     git2r     0.28.0
##                  gitcreds      0.1.1
##                       gld      2.6.2
##                   glmmTMB    1.0.2.1
##             GlobalOptions      0.1.2
##                   globals     0.14.0
##                      glue      1.4.2
##                       gmp      0.6-2
##                       gnm      1.1-1
##               googledrive      1.0.1
##              googlesheets      0.3.0
##             googlesheets4      0.2.0
##               GPArotation  2014.11-1
##                    gplots      3.1.1
##              graphlayouts      0.7.1
##                 gridExtra        2.3
##                 groundhog      1.1.0
##              groupedstats      2.0.0
##                       gsl      2.1-6
##                    gtable      0.3.0
##                    gtools      3.8.2
##               harrypotter      2.1.1
##                     haven      2.3.1
##                     headR      0.1.1
##                      here      1.0.1
##                     highr        0.8
##                     Hmisc      4.4-2
##                       hms      0.5.3
##                hrbrthemes      0.8.0
##                  htmldeps      0.1.1
##                 htmlTable      2.1.0
##                 htmltools      0.5.0
##               htmlwidgets      1.5.3
##                  httpcode      0.3.0
##                    httpuv      1.5.5
##                      httr      1.4.2
##                  hunspell      3.0.1
##                  hypergeo     1.2-13
##                       ids      1.0.1
##                    igraph      1.2.6
##                     infer      0.5.3
##                       ini      0.3.1
##                    inline     0.3.17
##                   insight     0.11.1
##                  installr     0.22.0
##                    ipmisc      5.0.2
##                      ISLR        1.2
##                   isoband      0.2.3
##                  ISOcodes 2020.12.04
##                 iterators     1.0.13
##               janeaustenr      0.1.5
##                   janitor      2.1.0
##                   jcolors      0.0.4
##                       jmv     1.2.23
##                jmvconnect     1.2.18
##                   jmvcore     1.2.23
##                      jomo      2.7-2
##                      jpeg    0.1-8.1
##                  jsonlite      1.7.2
##                  jspsychr 0.0.0.9000
##                kableExtra      1.3.1
##                     km.ci      0.5-2
##                    KMsurv      0.1-5
##                     knitr       1.30
##                    koRpus     0.13-4
##            koRpus.lang.en      0.1-4
##                  kSamples      1.2-9
##                  labeling      0.4.2
##                 languageR      1.5.0
##             LaplacesDemon     16.1.4
##                     later    1.1.0.1
##              latticeExtra     0.6-29
##                    lavaan      0.6-7
##                  lazyeval      0.2.2
##                   LCFdata        2.0
##                   leaflet    2.0.4.1
##         leaflet.providers      1.9.0
##                   libcoin      1.0-6
##                 lifecycle      0.2.0
##                   listenv      0.8.0
##                      lme4     1.1-26
##  LMERConvenienceFunctions        3.0
##                  lmerTest      3.1-3
##                      lmom        2.8
##                    lmtest     0.9-38
##                 logspline     2.1.16
##                       loo      2.4.1
##                   lsmeans     2.30-0
##                 lubridate    1.7.9.2
##                      lutz      0.3.1
##                    magick      2.5.2
##                  magrittr      2.0.1
##          manipulateWidget     0.10.1
##                   mapproj      1.2.7
##                      maps      3.3.0
##                  maptools      1.0-2
##                  markdown        1.1
##                  mathjaxr      1.0-1
##                matrixcalc      1.0-3
##              MatrixModels      0.4-1
##               matrixStats     0.57.0
##                   maxstat     0.7-25
##                     MBESS      4.8.0
##                      mc2d     0.1-18
##                   memoise      1.1.0
##                     MEMSS      0.9-3
##                   metaBMA      0.6.6
##                   metafor      2.4-0
##                  metaplus     0.7-11
##                        mi        1.0
##                      mice     3.12.0
##                      mime        0.9
##                    miniUI    0.1.1.1
##                     minqa      1.2.4
##                     mitml      0.3-7
##                    mnormt      2.0.2
##                 modeldata      0.1.0
##                    modelr      0.1.8
##                modeltools     0.2-23
##                moderndive      0.5.1
##                   moments       0.14
##                      MOTE      1.0.2
##                  multcomp     1.4-15
##              multcompView      0.1-8
##                     MuMIn    1.43.17
##                   munsell      0.5.0
##                mvnormtest      0.1-9
##                   mvtnorm      1.1-1
##                     ngram      3.1.0
##                     nlmeU     0.70-3
##                    nloptr    1.2.2.2
##                       NLP      0.2-1
##                   nortest      1.0-4
##                  numDeriv 2016.8-1.1
##              nycflights13      1.0.1
##                   officer     0.3.16
##                 oompaBase      3.2.9
##                    OpenMx     2.18.1
##                   openssl      1.4.3
##                  openxlsx      4.2.3
##            operator.tools      1.6.3
##                   ordinal 2019.12-10
##                      osfr      0.2.8
##                   packrat      0.5.0
##                PairedData      1.1.1
##       pairwiseComparisons      3.1.1
##                 paletteer      1.3.0
##                      palr      0.2.0
##                      pals        1.6
##                       pan        1.6
##                    pander      0.6.3
##                    papaja 0.1.0.9942
##                parallelly     1.23.0
##                parameters     0.10.1
##                 patchwork      1.1.1
##                   pbapply      1.4-3
##                  pbivnorm      0.6.0
##                  pbkrtest    0.5-0.1
##               performance      0.6.1
##                    pillar      1.4.7
##                  pkgbuild      1.2.0
##                 pkgconfig      2.0.3
##                   pkgload      1.1.0
##                       PKI      0.1-7
##                     plogr      0.2.0
##                   plotrix      3.7-8
##                   plumber      1.0.0
##                      plyr      1.8.6
##                     PMCMR        4.3
##                 PMCMRplus      1.7.1
##                       png      0.1-7
##                  polyclip     1.10-0
##                   polynom      1.4-0
##                     ppcor        1.1
##                    praise      1.0.0
##                    prereg      0.5.0
##               prettyunits      1.1.1
##                 prismatic      1.0.0
##                      pROC     1.16.2
##                  processx      3.4.5
##                  progress      1.2.2
##                  promises      1.1.1
##                     proto      1.0.0
##                        ps      1.5.0
##                     psych     2.0.12
##                     purrr      0.3.4
##                       pwr      1.3-0
##                 QuantPsyc        1.5
##                  quantreg       5.82
##                    qvcalc      1.0.2
##                   R.cache     0.14.0
##               R.methodsS3      1.8.1
##                      R.oo     1.24.0
##                   R.utils     2.10.1
##                        R6      2.5.0
##                randomizeR      2.0.0
##                  rappdirs      0.3.1
##                    raster      3.4-5
##                 rcmdcheck      1.3.3
##              RColorBrewer      1.1-2
##                rcompanion     2.3.26
##                      Rcpp      1.0.5
##             RcppArmadillo 0.10.1.2.2
##                 RcppEigen  0.3.3.9.1
##              RcppParallel      5.0.2
##                     RCurl   1.98-1.2
##                  readbulk      1.1.3
##                     readr      1.4.0
##                    readxl      1.3.1
##                RefManageR      1.3.0
##                    relimp      1.0-5
##                   rematch      1.0.1
##                  rematch2      2.1.2
##                   remotes      2.2.0
##                      repr      1.1.0
##                    reprex      0.3.0
##                   reshape      0.8.8
##                  reshape2      1.4.4
##                reticulate       1.18
##                  revealjs        0.9
##                       rex      1.2.0
##                       rgl   0.104.16
##               RgoogleMaps    1.4.5.3
##                       rio     0.5.16
##                     rJava     0.9-13
##                     rjson     0.2.20
##                   RJSONIO    1.3-1.4
##                     rlang     0.4.10
##                    RLRsim      3.1-6
##                 rmarkdown        2.6
##                  rmdfiltr      0.1.3
##                     Rmpfr      0.8-2
##                      ROCR     1.0-11
##                 rootSolve    1.8.2.1
##                    rorcid      0.6.4
##                  roxygen2      7.1.1
##                rpart.plot      3.0.9
##                       rpf      1.0.5
##                 rprojroot      2.0.2
##               rqdatatable      1.2.9
##                    rquery      1.4.6
##                   rsample      0.0.8
##                 rsconnect     0.8.16
##                 RSelenium      1.7.7
##                     rstan     2.21.2
##                rstantools      2.1.1
##                   rstatix      0.6.0
##                rstudioapi       0.13
##                   rticles       0.18
##                  Rttf2pt1      1.3.8
##                 rversions      2.0.2
##                     rvest      0.3.6
##                  sandwich      3.0-0
##                    scales      1.1.1
##                   scholar      0.2.0
##                     scico      1.2.0
##              scienceverse 0.0.0.9004
##                   selectr      0.4-2
##                       sem     3.1-11
##                  semTools      0.5-4
##                    semver      0.2.0
##                     servr       0.21
##               sessioninfo      1.1.1
##                     shape      1.4.5
##                     shiny      1.5.0
##            shinydashboard      0.7.1
##                   shinyjs      2.0.0
##               shinythemes      1.1.2
##                      sigr      1.1.3
##                      simr      1.0.5
##                sjlabelled      1.1.7
##                    sjmisc      2.8.6
##                    sjPlot      2.8.7
##                   sjstats     0.18.1
##                     skimr      2.1.2
##                      slam     0.1-48
##                    slider      0.1.5
##                        sm    2.2-5.6
##                 snakecase     0.11.0
##                 SnowballC      0.7.0
##                    sodium        1.1
##               sourcetools      0.1.7
##                        sp      1.4-5
##                      spam      2.6-0
##                   SparseM       1.78
##               StanHeaders   2.21.0-7
##                   statmod     1.4.35
##          statsExpressions      0.6.2
##                 stopwords        2.1
##                   stringi      1.5.3
##                   stringr      1.4.0
##                subprocess      0.8.3
##                Superpower      0.1.0
##                 SuppDists    1.1-9.5
##                 survminer      0.4.8
##                  survMisc      0.5.5
##                   swagger     3.33.1
##                  sylcount      0.2-2
##                     sylly      0.1-6
##                  sylly.en      0.1-3
##                       sys        3.4
##               systemfonts      0.3.2
##                    tables      0.9.6
##                       tau     0.0-23
##                  testthat      3.0.1
##                   textcat      1.0-7
##                   TH.data     1.0-10
##                    tibble      3.0.4
##                    tidyBF      0.4.2
##                 tidygraph      1.2.0
##                     tidyr      1.1.2
##                   tidyRSS      2.0.3
##                tidyselect      1.1.0
##                  tidytext      0.3.0
##                 tidyverse      1.3.0
##                timesavers      0.1.2
##                   tinytex       0.28
##                       TMB     1.7.18
##                   tmvnsim      1.0-2
##                tokenizers      0.2.1
##                    TOSTER      0.3.4
##                translateR        1.0
##                 triebeard      0.3.0
##                 truncnorm      1.0-8
##                    tweenr      1.0.1
##                    ucminf      1.1-4
##                  urltools      1.7.3
##                   usethis      2.0.0
##                      utf8      1.1.4
##                      uuid      0.1-4
##                        V8      3.4.0
##                       vcd      1.4-8
##                  vcdExtra      0.7-1
##                     vctrs      0.3.6
##                   viridis      0.5.1
##               viridisLite      0.3.0
##                     waldo      0.2.3
##                      warp      0.2.0
##                     wdman      0.2.5
##                   webshot      0.5.2
##                  webutils        1.1
##               wesanderson      0.3.6
##                   whisker        0.4
##                     withr      2.3.0
##                 wordcloud        2.6
##            wordcountaddin 0.3.0.9000
##                 workflowr      1.6.2
##                     wrapr      2.0.6
##                      WRS2      1.1-0
##                   WVPlots      1.3.2
##                   WWGbook      1.0.1
##                  xaringan       0.19
##                      xfun       0.20
##                 XLConnect      1.0.1
##             XLConnectJars     0.2-15
##                      xlsx      0.6.5
##                  xlsxjars      0.6.1
##                       XML   3.99-0.5
##                      xml2      1.3.2
##                     xopen      1.0.0
##                    xtable      1.8-4
##                       xts     0.12.1
##                      yaml      2.2.1
##                   zeallot      0.1.0
##                       zip      2.1.1
##                       zoo      1.8-8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;There are &amp;quot;,dim(ip)[1], &amp;quot; packages installed in my laptop.&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;There are 513 packages installed in my laptop.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Richard’s book inspired me help people control their golems/ststistical models in the process of coding. In his book, literature and codes are separated. Readers who are not familiar with coding skill might hardly follow his literature. Literatural coding might be the best way to impliment the &lt;code&gt;Rethinking&lt;/code&gt;. I am going to accumulating the codes of Bayesian statistics and thake notes of his and others literatures in the coming posts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Display Math Equation in Post</title>
      <link>https://scchen.com/post/2016-04-05-display-math-equation-in-post/</link>
      <pubDate>Tue, 05 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-04-05-display-math-equation-in-post/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There will be many posts demostrate how to transfer the mathematical laws to the codes, and the reversed flaw. I start from this famous equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E = mc^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Use the steps 1 and 2 indtroducted in &lt;a href=&#34;http://tinyheero.github.io/2015/12/06/rmd-to-jekyll-protect-eqn.html&#34;&gt;Fong Chun Chan’s post&lt;/a&gt;, my website is able to show the equation in any post. Because this theme use &lt;code&gt;kramdown&lt;/code&gt; as the engine of markdown, I pass the step 4.&lt;/p&gt;
&lt;p&gt;Here are the proof the central limitation theorem. Mix of inline and one-line equations.&lt;br /&gt;
We have a sequence of independent random variables,&lt;span class=&#34;math display&#34;&gt;\[ X_1, X_2, \ldots \]&lt;/span&gt;&lt;br /&gt;
And the mean and variance of them:&lt;br /&gt;
Mean &lt;span class=&#34;math display&#34;&gt;\[ E \left[{X_i}\right] = \mu \in \left({-\infty \,.\,.\, \infty}\right) \]&lt;/span&gt;&lt;br /&gt;
Variance &lt;span class=&#34;math display&#34;&gt;\[ V \left({X_i}\right) = \sigma^2 &amp;gt; 0 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Assume:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ S_n = \sum_{i \mathop = 1}^n X_i \]&lt;/span&gt;&lt;br /&gt;
Then:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \displaystyle \frac {S_n - n \mu} {\sqrt {n \sigma^2} } \xrightarrow {D} N \left({0, 1}\right)\]&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[ n \to \infty \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Embedded tex codes in &lt;code&gt;$$ ... $$&lt;/code&gt;, the equation was transfered inline or in single line. Thus I pass the step 3 of &lt;a href=&#34;http://tinyheero.github.io/2015/12/06/rmd-to-jekyll-protect-eqn.html&#34;&gt;Fong Chun Chan’s method&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note.&lt;/strong&gt; Justify the last equation in 2021-01-19 17:54:52&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vectors in mathematics and in codes</title>
      <link>https://scchen.com/post/2016-04-05-vectors-in-mathematics-and-in-codes/</link>
      <pubDate>Tue, 05 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-04-05-vectors-in-mathematics-and-in-codes/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When a set of data/observations is imported to R, it is usually treated as &lt;code&gt;vector&lt;/code&gt;. Vector has two mathematical forms as following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;column vector&lt;/strong&gt;:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \vec{a} = \begin{bmatrix}
 1\\
 2\\
 3\\
\end{bmatrix} \]&lt;/span&gt;&lt;br /&gt;
&lt;strong&gt;row vector&lt;/strong&gt;:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \vec{a} = [ 1\ 2\ 3 ] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Default form of vector in R is column vector. More precisely, it is treated as a matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 1:3
a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  int [1:3] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a statistical work, we used to calcuate the sum of square (SS) for the deviations to mean. The common equation is like:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \sum_{i=1}^{n}(Y_{i}-\overline{Y})^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the fundemental for calculating the variance of this set of data/observations. The equation of variance is like this:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \frac{\sum_{i=1}^{n}(Y_{i}-\overline{Y})^2}{n} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because the data/observations are treated as matrix, we are able to have the sum of square by the multiplication of this matrix.&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \mathbf{A&amp;#39;A} = 
\begin{bmatrix} A_1 A_2 A_3 \dots\end{bmatrix}
\times
\begin{bmatrix}
   A_1 \\
   A_2 \\
   A_3 \\
   \vdots
\end{bmatrix}　\]&lt;/span&gt;　　&lt;/p&gt;
&lt;p&gt;Here are ten observations 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, the average of this data is 5.5. Let’s have the SS and variance in use of the multiplication of matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SS &amp;lt;- t(1:10 - mean(1:10)) %*% (1:10 - mean(1:10)) 
VAR &amp;lt;- SS/length(1:10)
SS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,] 82.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VAR&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,] 8.25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In R &lt;code&gt;base&lt;/code&gt; package, the function &lt;code&gt;var&lt;/code&gt; outputs a sampling variance, not a population variance as above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9.166667&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Sampling Distribution in R Programming</title>
      <link>https://scchen.com/post/2016-03-30-learning-sampling-distribution-in-r-programming/</link>
      <pubDate>Wed, 30 Mar 2016 00:00:00 +0000</pubDate>
      <guid>https://scchen.com/post/2016-03-30-learning-sampling-distribution-in-r-programming/</guid>
      <description>
&lt;script src=&#34;https://scchen.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Sampling distribution is the set of possible outcomes when we collect data through the randomization procedure (random sampling, ramond assignment). Do a simulation work is the best way to understand the sampling distribution. A simulation work is unrelated to any context we collect the data. You can connect the simulation work to any randominzation in the real world.&lt;/p&gt;
&lt;p&gt;In this pseudo experiment, there are only ten observations we will collect in every sample. They are 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Accodring to the design of the experiment, a sample will have one observation to countless observations. I assume the distrubtions are accumulated from 100 samples of 1 observation, 9 observations, ,16 observations, 25 observations, and 36 observations. Every sample will be collapsed to a average value and become the components of sampling distribution. I use &lt;code&gt;ggplot2&lt;/code&gt; to draw the five sampling distributions. Look at what they are look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages({
  library(ggplot2)
  library(xtable)
  })

set.seed(1)
OBV &amp;lt;- 1:10
Dist1 &amp;lt;- NULL
Dist9 &amp;lt;- NULL
Dist16 &amp;lt;- NULL
Dist25 &amp;lt;- NULL
Dist36 &amp;lt;- NULL
count = 100
while(count &amp;gt; 0){Dist1 &amp;lt;- c(Dist1,sample(OBV, 1, replace = TRUE)); count &amp;lt;- count - 1}
count = 100
while(count &amp;gt; 0){Dist9 &amp;lt;- c(Dist9,mean(sample(OBV, 9,replace = TRUE) ) ); count &amp;lt;- count - 1}
count = 100
while(count &amp;gt; 0){Dist16 &amp;lt;- c(Dist16,mean(sample(OBV, 16,replace = TRUE) ) ); count &amp;lt;- count - 1}
count = 100
while(count &amp;gt; 0){Dist25 &amp;lt;- c(Dist25,mean(sample(OBV, 25,replace = TRUE) ) ); count &amp;lt;- count - 1}
count = 100
while(count &amp;gt; 0){Dist36 &amp;lt;- c(Dist36,mean(sample(OBV, 36,replace = TRUE) ) ); count &amp;lt;- count - 1}
Dist.df &amp;lt;- data.frame(Size = factor(rep(c(1,9,16,25,36), each=100)), Sample_Means = c(Dist1, Dist9, Dist16, Dist25, Dist36) )
ggplot(Dist.df, aes(Sample_Means, fill = Size)) + geom_histogram() + facet_grid(. ~ Size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://scchen.com/en/post/2016-03-30-learning-sampling-distribution-in-r-programming_files/figure-html/Sampling-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We call the set of observations 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 &lt;strong&gt;population&lt;/strong&gt; in any circumstance we conduct this experiment. This population has the average 5.5 and the variance/standard deviation 8.25/2.87. With the increase of sample size, you find more and more samples collapsed to the average of population. The variation of each sample distribution decreases with the increasing of sample size as well. The following table illustrate the average and variance/standard deviation of each sampling distribution.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Sample Size&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Variance&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Standard Deviation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6.06&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;8.14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.85&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.62&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.91&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.58&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.54&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.43&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.66&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.51&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.52&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are three findings in this simulation. First, the average of every sample is as equal as the average of population. Second, the variance of every sample is close to the divide of population variance by the sample size. Third, the standard deviation of every sample is close to the divide of population standard deviation by the square of sample size. These facts matches &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34;&gt;Central limit theorem&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lorem Ipsum</title>
      <link>https://scchen.com/post/2015-01-01-lorem-ipsum/</link>
      <pubDate>Thu, 01 Jan 2015 13:09:13 -0600</pubDate>
      <guid>https://scchen.com/post/2015-01-01-lorem-ipsum/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Lorem ipsum&lt;/strong&gt; dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore &lt;em&gt;magna aliqua&lt;/em&gt;. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
