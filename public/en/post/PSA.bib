
@article{KleinInvestigatingVariationReplicability2014,
  title = {Investigating {{Variation}} in {{Replicability}}},
  volume = {45},
  doi = {10.1027/1864-9335/a000178},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect \textendash{} imagined contact reducing prejudice \textendash{} showed weak support for replicability. And two effects \textendash{} flag priming influencing conservatism and currency priming influencing system justification \textendash{} did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  journal = {Social Psychology},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams Jr, Reginald B. and Bahn{\'\i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and Ijzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {01/01/ 2014},
  pages = {142-152},
  file = {D:\\CORE\\Version_Controls\\zotero_data\\storage\\2HB77DG8\\Many_lab.pdf}
}

@article{ZwaanRevisitingMentalSimulation2012,
  title = {Revisiting {{Mental Simulation}} in {{Language Comprehension}}: {{Six Replication Attempts}}},
  volume = {7},
  doi = {10.1371/journal.pone.0051382},
  abstract = {The notion of language comprehension as mental simulation has become popular in cognitive science. We revisit some of the original empirical evidence for this. Specifically, we attempted to replicate the findings from earlier studies that examined the mental simulation of object orientation, shape, and color, respectively, in sentence-picture verification. For each of these sets of findings, we conducted two web-based replication attempts using Amazon's Mechanical Turk. Our results are mixed. Participants responded faster to pictures that matched the orientation or shape implied by the sentence, replicating the original findings. The effect was larger and stronger for shape than orientation. Participants also responded faster to pictures that matched the color implied by the sentence, whereas the original studies obtained \emph{mis}match advantages. We argue that these results support mental simulation theory, show the importance of replication studies, and show the viability of web-based data collection.},
  journal = {PLoS ONE},
  author = {Zwaan, Rolf A. and Pecher, Diane},
  year = {2012},
  pages = {e51382},
  file = {D:\\CORE\\Version_Controls\\zotero_data\\storage\\EVA2DD79\\journal.pone.0051382.pdf;D:\\CORE\\Version_Controls\\zotero_data\\storage\\Z6I2VBEB\\Structure_of_the_Excel_files.pdf}
}

@article{GlenbergGroundinglanguageaction2002,
  title = {Grounding Language in Action},
  volume = {9},
  issn = {1069-9384},
  doi = {10.3758/bf03196313},
  language = {English},
  journal = {Psychonomic Bulletin \& Review},
  author = {Glenberg, ArthurM and Kaschak, MichaelP},
  month = sep,
  year = {2002},
  pages = {558-565},
  file = {D:\\CORE\\reading\\Psychonomic B&R\\10.3758_BF03196313.pdf}
}

@article{BarsalouPerceptualsymbolsystems1999,
  title = {Perceptual Symbol Systems},
  volume = {22},
  doi = {10.1017/S0140525X99002149},
  journal = {Behavioral and Brain Sciences},
  author = {Barsalou, Lawrence W.},
  year = {1999},
  pages = {577-660},
  file = {D:\\CORE\\Version_Controls\\zotero_data\\storage\\24WTCTI6\\Barsalou_BBS_1999_perceptual_symbol_systems.pdf}
}

@article{ioannidis_why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  volume = {2},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Published research findings are sometimes refuted by subsequent evidence, says Ioannidis, with ensuing confusion and disappointment.},
  number = {8},
  journal = {PLOS Med},
  author = {Ioannidis, John P. A.},
  month = aug,
  year = {2005},
  keywords = {meta-analysis,Research design,Randomized controlled trials,Genetic epidemiology,Genetics of disease,Schizophrenia,Clinical research design,Finance},
  pages = {e124},
  file = {D:\\CORE\\reading\\PLOS\\journal.pmed.0020124.pdf}
}

@article{stanfield_effect_2001,
  title = {The Effect of Implied Orientation Derived from Verbal Context on Picture Recognition},
  volume = {12},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00326},
  abstract = {Perceptual symbol systems assume an analogue relationship between a symbol and its referent, whereas amodal symbol systems assume an arbitrary relationship between a symbol and its referent. According to perceptual symbol theories, the complete representation of an object, called a simulation, should reflect physical characteristics of the object. Amodal theories, in contrast, do not make this prediction. We tested the hypothesis, derived from perceptual symbol theories, that people mentally represent the orientation of an object implied by a verbal description. Orientation (vertical-horizontal) was manipulated by having participants read a sentence that implicitly suggested a particular orientation for an object. Then recognition latencies to pictures of the object in each of the two orientations were measured. Pictures matching the orientation of the object implied by the sentence were responded to faster than pictures that did not match the orientation. This finding is interpreted as offering support for theories positing perceptual symbol systems.},
  language = {eng},
  number = {2},
  journal = {Psychological Science},
  author = {Stanfield, R. A. and Zwaan, R. A.},
  month = mar,
  year = {2001},
  keywords = {Adult,Association Learning,Cues,Female,Form Perception,Humans,Male,Memory,Models; Psychological,Perception,Recognition (Psychology)},
  pages = {153-156},
  file = {D:\\CORE\\reading\\psychological science\\Psychological Science-2001-Stanfield-153-6.pdf},
  pmid = {11340925}
}

@article{OpenScienceCollaborationEstimatingreproducibilitypsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  volume = {349},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P $<$ .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. View larger version: In this page In a new window Download PowerPoint Slide for Teaching Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716},
  language = {en},
  number = {6251},
  journal = {Science},
  author = {{Open Science Collaboration}},
  month = aug,
  year = {2015},
  pages = {aac4716},
  file = {D:\\CORE\\Version_Controls\\zotero_data\\storage\\FTAWWJZ2\\100-percent-reproducibility.html;D:\\CORE\\Version_Controls\\zotero_data\\storage\\S4J2SR8W\\aac4716.html},
  pmid = {26315443}
}

@article{EbersoleManyLabsEvaluating2016,
  series = {Special Issue: Confirmatory},
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  volume = {67},
  issn = {0022-1031},
  shorttitle = {Many {{Labs}} 3},
  doi = {10.1016/j.jesp.2015.10.012},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N = 2696) and with an online sample (N = 737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences\textemdash{}conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  journal = {Journal of Experimental Social Psychology},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and Joy-Gaba, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and {van Allen}, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  month = nov,
  year = {2016},
  keywords = {replication,Cognitive Psychology,social psychology,individual differences,Participant pool,Sampling effects,Situational effects},
  pages = {68-82},
  file = {D:\\CORE\\Version_Controls\\zotero_data\\storage\\SGQE5INN\\Ebersole 等。 - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf;D:\\CORE\\Version_Controls\\zotero_data\\storage\\CS8IEUEU\\S0022103115300123.html}
}

@article{FanelliPositiveResultsIncrease2010,
  title = {``{{Positive}}'' {{Results Increase Down}} the {{Hierarchy}} of the {{Sciences}}},
  volume = {5},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the ``hardness'' of scientific research\textemdash{}i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors\textemdash{}is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a ``positive'' (full or partial) or ``negative'' support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in ``softer'' sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  number = {4},
  journal = {PLOS ONE},
  author = {Fanelli, Daniele},
  month = apr,
  year = {2010},
  keywords = {Psychology,Scientists,Social sciences,Behavior,Social research,Physical sciences,Sociology,Mental health and psychiatry},
  pages = {e10068},
  file = {D:\\CORE\\Version_Controls\\zotero_data\\storage\\9Z5AESGS\\Fanelli - 2010 - “Positive” Results Increase Down the Hierarchy of .pdf;D:\\CORE\\Version_Controls\\zotero_data\\storage\\3TH838AV\\article.html}
}

@article{MoshontzPsychologicalScienceAccelerator2018,
  title = {The {{Psychological Science Accelerator}}: {{Advancing Psychology}} through a {{Distributed Collaborative Network}}},
  shorttitle = {The {{Psychological Science Accelerator}}},
  doi = {10.17605/OSF.IO/785QU},
  abstract = {Concerns have been growing about the veracity of psychological findings. Many findings in psychological science are based on studies with insufficient statistical power and non-representative samples, or may otherwise be limited to specific, ungeneralizable settings or populations. Large-scale collaboration, in which one or more research projects are conducted across multiple lab sites, offers a pragmatic solution to these and other current methodological challenges. The Psychological Science Accelerator (PSA) is a distributed network of laboratories designed to enable and support crowdsourced research projects. The PSA's mission is to accelerate the accumulation of reliable and generalizable evidence in psychological science. Here, we describe the background, structure, principles, procedures, benefits, and challenges of the PSA. In contrast to other crowdsourced research networks, the PSA is ongoing (as opposed to time-limited), efficient (in terms of re-using structures and principles for different projects), decentralized, diverse (in terms of participants and researchers), and inclusive (of proposals, contributions, and other relevant input from anyone inside or outside of the network). The PSA and other approaches to crowdsourced psychological science will advance our understanding of mental processes and behaviors by enabling rigorous research and systematically examining its generalizability.},
  journal = {PsyArXiv},
  author = {Moshontz, Hannah and Campbell, Lorne and Ebersole, Charles R. and IJzerman, Hans and Urry, Heather L. and Forscher, Patrick S. and Grahe, Jon and McCarthy, Randy J. and Musser, Erica D. and Protzko and Flake, Jessica Kay and Forero, Diego A. and Janssen, Steve M. J. and Keene, Justin and Aczel, Balazs and Ansari, Daniel and Antfolk, Jan and Baskin, Ernest and Batres, Carlota and Lucia, Martha and Brick, Cameron and Castille, Christopher Michael and Chandel, Priyanka and Chopik, William J. and Clarance, David and Corker, Katherine S. and Dixson, Barnaby and Dranseika, Vilius and Dunham, Yarrow and Evans, Thomas Rhys and Fiedler, Susann and Fu, Cynthia H. Y. and Gardiner, Gwen and Garrison, S. Mason and Gill, Tripat and Hahn, Amanda and Jaeger, Bastian and Ka{\v c}m{\'a}r, Pavol and Gwena{\"e}l, Kaminski and Kanske, Philipp and Kekecs, Zoltan and Kline, Melissa and Koehn, Monica A. and Kujur, Pratibha and Levitan, Carmel and Miller, Jeremy K. and Okan, Ceylan and Olsen, Jerome and Oviedo-Trespalacios, Oscar and Ozdogru, Asil and Pande, Babita and Parganiha, Arti and Parveen, Noorshama and Pfuhl, Gerit and Pradhan, Sraddha and Wilson, John Paul and Ropovik, Ivan and Saunders, Blair and Schei, Vidar and Schmidt, Kathleen and Singh, Margaret Messiah and Sirota, Miroslav and Solas, Sara {\'A}lvarez and Steltenpohl, Crystal and Stieger, Stefan and Storage, Daniel and Sullivan, Gavin Brent and Szabelska, Anna and Tamnes, Christian Krog and Vadillo, Miguel A. and Vanpaemel, Wolf and Vergauwe, Evie and Verschoor, Mark and Vianello, Michelangelo and Voracek, Martin and Zickfeld, Janis Heinrich and Awlia, Dana and Chatard, Armand and Fernandez, Ana Maria and Kapucu, Aycan and Mensink, Michael and Chartier, Christopher R.},
  month = apr,
  year = {2018},
  file = {D:\\CORE\\reading\\preprints\\Moshontz 等。 - 2018 - The Psychological Science Accelerator Advancing P.pdf;D:\\CORE\\Version_Controls\\zotero_data\\storage\\6LXEVRW7\\785qu.html}
}

@article{ReifmanIntroductionSpecialIssue2016,
  title = {Introduction to the {{Special Issue}} of {{Emerging Adulthood}}},
  volume = {4},
  issn = {2167-6968, 2167-6984},
  doi = {10.1177/2167696815588022},
  language = {en},
  number = {3},
  journal = {Emerging Adulthood},
  author = {Reifman, Alan and Grahe, Jon E.},
  month = jun,
  year = {2016},
  pages = {135-141}
}

@techreport{Frankcollaborativeapproachinfant2016,
  title = {A Collaborative Approach to Infant Research: {{Promoting}} Reproducibility, Best Practices, and Theory-Building},
  shorttitle = {A Collaborative Approach to Infant Research},
  abstract = {The ideal of scientific progress is that we accumulate measurements and integrate these into theory, but recent discussion of replicability issues has cast doubt on whether psychological research conforms to this model. Developmental research \textendash{} especially with infant participants \textendash{} also has discipline-specific replicability challenges, including small samples due to high recruitment costs and limited measurement methods. Inspired by collaborative replication efforts in cognitive and social psychology, we describe a proposal for assessing and promoting replicability in infancy research: large-scale, multi-lab replication efforts aiming for a more precise understanding of key developmental phenomena. The ManyBabies project, our instantiation of this proposal, will not only help us estimate how robust and replicable these phenomena are, but also gain new theoretical insights into how they vary across ages, linguistic communities, and measurement methods. This project has the potential for a variety of positive outcomes, including less-biased estimates of theoretically-important effects, estimates of variability that can be used for later study planning, and a series of best-practices blueprints for future infancy research.},
  institution = {{PsyArXiv}},
  author = {Frank, Michael},
  year = {2016},
  doi = {10.17605/OSF.IO/27B43}
}


